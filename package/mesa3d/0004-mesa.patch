diff --git a/docs/envvars.rst b/docs/envvars.rst
index 56550ec658d..a81a5843039 100644
--- a/docs/envvars.rst
+++ b/docs/envvars.rst
@@ -596,17 +596,17 @@ LLVMpipe driver environment variables
 VMware SVGA driver environment variables
 ----------------------------------------
 
-:envvar`SVGA_FORCE_SWTNL`
+:envvar:`SVGA_FORCE_SWTNL`
    force use of software vertex transformation
-:envvar`SVGA_NO_SWTNL`
+:envvar:`SVGA_NO_SWTNL`
    don't allow software vertex transformation fallbacks (will often
    result in incorrect rendering).
-:envvar`SVGA_DEBUG`
+:envvar:`SVGA_DEBUG`
    for dumping shaders, constant buffers, etc. See the code for details.
-:envvar`SVGA_EXTRA_LOGGING`
+:envvar:`SVGA_EXTRA_LOGGING`
    if set, enables extra logging to the ``vmware.log`` file, such as the
    OpenGL program's name and command line arguments.
-:envvar`SVGA_NO_LOGGING`
+:envvar:`SVGA_NO_LOGGING`
    if set, disables logging to the ``vmware.log`` file. This is useful
    when using Valgrind because it otherwise crashes when initializing
    the host log feature.
diff --git a/meson.build b/meson.build
index 250b52886e5..cae1e4b5834 100644
--- a/meson.build
+++ b/meson.build
@@ -193,7 +193,7 @@ if gallium_drivers.contains('auto')
       ]
     elif ['arm', 'aarch64'].contains(host_machine.cpu_family())
       gallium_drivers = [
-        'v3d', 'vc4', 'freedreno', 'etnaviv', 'nouveau',
+        'v3d', 'vc4', 'freedreno', 'etnaviv', 'nouveau', 'svga',
         'tegra', 'virgl', 'lima', 'panfrost', 'swrast'
       ]
     elif ['mips', 'mips64', 'riscv32', 'riscv64'].contains(host_machine.cpu_family())
@@ -1903,12 +1903,8 @@ with_opencl_native = _opencl != 'disabled' and get_option('opencl-native')
 if (with_amd_vk or with_gallium_radeonsi or
     (with_gallium_opencl and with_opencl_native) or
     (with_gallium_r600 and with_llvm))
-  if with_platform_windows
-    dep_elf = dependency('libelf', required : false, fallback : ['libelf', 'libelf_dep'])
-  else
-    dep_elf = dependency('libelf', required : false)
-  endif
-  if not dep_elf.found()
+  dep_elf = dependency('libelf', required : false)
+  if not with_platform_windows and not dep_elf.found()
     dep_elf = cc.find_library('elf')
   endif
 else
diff --git a/src/amd/common/ac_binary.c b/src/amd/common/ac_binary.c
index 96c83fa8a71..a22392efb48 100644
--- a/src/amd/common/ac_binary.c
+++ b/src/amd/common/ac_binary.c
@@ -27,8 +27,10 @@
 #include "util/u_math.h"
 #include "util/u_memory.h"
 
+#ifndef _WIN32
 #include <gelf.h>
 #include <libelf.h>
+#endif
 #include <sid.h>
 #include <stdio.h>
 
diff --git a/src/amd/common/ac_rgp.c b/src/amd/common/ac_rgp.c
index e0180b6e1c6..e319d96a524 100644
--- a/src/amd/common/ac_rgp.c
+++ b/src/amd/common/ac_rgp.c
@@ -1002,6 +1002,7 @@ static void ac_sqtt_dump_spm(const struct ac_spm_trace_data *spm_trace,
    fseek(output, file_offset, SEEK_SET);
 }
 
+#ifndef _WIN32
 static void ac_sqtt_dump_data(struct radeon_info *rad_info,
                               struct ac_thread_trace *thread_trace,
                               const struct ac_spm_trace_data *spm_trace,
@@ -1188,11 +1189,15 @@ static void ac_sqtt_dump_data(struct radeon_info *rad_info,
       ac_sqtt_dump_spm(spm_trace, file_offset, output);
    }
 }
+#endif
 
 int ac_dump_rgp_capture(struct radeon_info *info,
                         struct ac_thread_trace *thread_trace,
                         const struct ac_spm_trace_data *spm_trace)
 {
+#ifdef _WIN32
+   return -1;
+#else
    char filename[2048];
    struct tm now;
    time_t t;
@@ -1215,4 +1220,5 @@ int ac_dump_rgp_capture(struct radeon_info *info,
 
    fclose(f);
    return 0;
+#endif
 }
diff --git a/src/amd/common/meson.build b/src/amd/common/meson.build
index 0b511b534a0..2e6e1daf176 100644
--- a/src/amd/common/meson.build
+++ b/src/amd/common/meson.build
@@ -73,8 +73,6 @@ amd_common_files = files(
   'ac_shader_util.h',
   'ac_gpu_info.c',
   'ac_gpu_info.h',
-  'ac_rtld.c',
-  'ac_rtld.h',
   'ac_surface.c',
   'ac_surface.h',
   'ac_debug.c',
@@ -89,7 +87,6 @@ amd_common_files = files(
   'ac_rgp.h',
   'ac_msgpack.c',
   'ac_msgpack.h',
-  'ac_rgp_elf_object_pack.c',
   'ac_nir.c',
   'ac_nir.h',
   'ac_nir_opt_outputs.c',
@@ -104,6 +101,14 @@ amd_common_files = files(
   'ac_perfcounter.h',
 )
 
+if not with_platform_windows
+  amd_common_files += files(
+    'ac_rtld.c',
+    'ac_rtld.h',
+    'ac_rgp_elf_object_pack.c',
+  )
+endif
+
 libamd_common = static_library(
   'amd_common',
   [amd_common_files, sid_tables_h, amdgfxregs_h, gfx10_format_table_c],
diff --git a/src/amd/compiler/README-ISA.md b/src/amd/compiler/README-ISA.md
index d78cf7bdc5c..f27927615b0 100644
--- a/src/amd/compiler/README-ISA.md
+++ b/src/amd/compiler/README-ISA.md
@@ -220,7 +220,7 @@ VMEM/FLAT/GLOBAL/SCRATCH/DS instruction reads an SGPR (or EXEC, or M0).
 Then, a SALU/SMEM instruction writes the same SGPR.
 
 Mitigated by:
-A VALU instruction or an `s_waitcnt vmcnt(0)` between the two instructions.
+A VALU instruction or an `s_waitcnt` between the two instructions.
 
 ### SMEMtoVectorWriteHazard
 
diff --git a/src/amd/compiler/aco_insert_NOPs.cpp b/src/amd/compiler/aco_insert_NOPs.cpp
index 6d07813b736..01012a78e4c 100644
--- a/src/amd/compiler/aco_insert_NOPs.cpp
+++ b/src/amd/compiler/aco_insert_NOPs.cpp
@@ -159,6 +159,8 @@ struct NOP_ctx_gfx10 {
    bool has_NSA_MIMG = false;
    bool has_writelane = false;
    std::bitset<128> sgprs_read_by_VMEM;
+   std::bitset<128> sgprs_read_by_VMEM_store;
+   std::bitset<128> sgprs_read_by_DS;
    std::bitset<128> sgprs_read_by_SMEM;
 
    void join(const NOP_ctx_gfx10& other)
@@ -172,6 +174,8 @@ struct NOP_ctx_gfx10 {
       has_NSA_MIMG |= other.has_NSA_MIMG;
       has_writelane |= other.has_writelane;
       sgprs_read_by_VMEM |= other.sgprs_read_by_VMEM;
+      sgprs_read_by_DS |= other.sgprs_read_by_DS;
+      sgprs_read_by_VMEM_store |= other.sgprs_read_by_VMEM_store;
       sgprs_read_by_SMEM |= other.sgprs_read_by_SMEM;
    }
 
@@ -182,6 +186,8 @@ struct NOP_ctx_gfx10 {
              has_DS == other.has_DS && has_branch_after_DS == other.has_branch_after_DS &&
              has_NSA_MIMG == other.has_NSA_MIMG && has_writelane == other.has_writelane &&
              sgprs_read_by_VMEM == other.sgprs_read_by_VMEM &&
+             sgprs_read_by_DS == other.sgprs_read_by_DS &&
+             sgprs_read_by_VMEM_store == other.sgprs_read_by_VMEM_store &&
              sgprs_read_by_SMEM == other.sgprs_read_by_SMEM;
    }
 };
@@ -582,6 +588,16 @@ mark_read_regs(const aco_ptr<Instruction>& instr, std::bitset<N>& reg_reads)
    }
 }
 
+template <std::size_t N>
+void
+mark_read_regs_exec(State& state, const aco_ptr<Instruction>& instr, std::bitset<N>& reg_reads)
+{
+   mark_read_regs(instr, reg_reads);
+   reg_reads.set(exec);
+   if (state.program->wave_size == 64)
+      reg_reads.set(exec_hi);
+}
+
 bool
 VALU_writes_sgpr(aco_ptr<Instruction>& instr)
 {
@@ -638,31 +654,36 @@ handle_instruction_gfx10(State& state, NOP_ctx_gfx10& ctx, aco_ptr<Instruction>&
    // TODO: s_dcache_inv needs to be in it's own group on GFX10
 
    /* VMEMtoScalarWriteHazard
-    * Handle EXEC/M0/SGPR write following a VMEM instruction without a VALU or "waitcnt vmcnt(0)"
+    * Handle EXEC/M0/SGPR write following a VMEM/DS instruction without a VALU or "waitcnt vmcnt(0)"
     * in-between.
     */
    if (instr->isVMEM() || instr->isFlatLike() || instr->isDS()) {
-      /* Remember all SGPRs that are read by the VMEM instruction */
-      mark_read_regs(instr, ctx.sgprs_read_by_VMEM);
-      ctx.sgprs_read_by_VMEM.set(exec);
-      if (state.program->wave_size == 64)
-         ctx.sgprs_read_by_VMEM.set(exec_hi);
+      /* Remember all SGPRs that are read by the VMEM/DS instruction */
+      if (instr->isVMEM() || instr->isFlatLike())
+         mark_read_regs_exec(
+            state, instr,
+            instr->definitions.empty() ? ctx.sgprs_read_by_VMEM_store : ctx.sgprs_read_by_VMEM);
+      if (instr->isFlat() || instr->isDS())
+         mark_read_regs_exec(state, instr, ctx.sgprs_read_by_DS);
    } else if (instr->isSALU() || instr->isSMEM()) {
       if (instr->opcode == aco_opcode::s_waitcnt) {
-         /* Hazard is mitigated by "s_waitcnt vmcnt(0)" */
-         uint16_t imm = instr->sopp().imm;
-         unsigned vmcnt = (imm & 0xF) | ((imm & (0x3 << 14)) >> 10);
-         if (vmcnt == 0)
+         wait_imm imm(state.program->gfx_level, instr->sopp().imm);
+         if (imm.vm == 0)
             ctx.sgprs_read_by_VMEM.reset();
-      } else if (instr->opcode == aco_opcode::s_waitcnt_depctr) {
+      } else if (instr->opcode == aco_opcode::s_waitcnt_depctr && instr->sopp().imm == 0xffe3) {
          /* Hazard is mitigated by a s_waitcnt_depctr with a magic imm */
-         if (instr->sopp().imm == 0xffe3)
-            ctx.sgprs_read_by_VMEM.reset();
+         ctx.sgprs_read_by_VMEM.reset();
+         ctx.sgprs_read_by_DS.reset();
+         ctx.sgprs_read_by_VMEM_store.reset();
       }
 
       /* Check if SALU writes an SGPR that was previously read by the VALU */
-      if (check_written_regs(instr, ctx.sgprs_read_by_VMEM)) {
+      if (check_written_regs(instr, ctx.sgprs_read_by_VMEM) ||
+          check_written_regs(instr, ctx.sgprs_read_by_DS) ||
+          check_written_regs(instr, ctx.sgprs_read_by_VMEM_store)) {
          ctx.sgprs_read_by_VMEM.reset();
+         ctx.sgprs_read_by_DS.reset();
+         ctx.sgprs_read_by_VMEM_store.reset();
 
          /* Insert s_waitcnt_depctr instruction with magic imm to mitigate the problem */
          aco_ptr<SOPP_instruction> depctr{
@@ -674,6 +695,8 @@ handle_instruction_gfx10(State& state, NOP_ctx_gfx10& ctx, aco_ptr<Instruction>&
    } else if (instr->isVALU()) {
       /* Hazard is mitigated by any VALU instruction */
       ctx.sgprs_read_by_VMEM.reset();
+      ctx.sgprs_read_by_DS.reset();
+      ctx.sgprs_read_by_VMEM_store.reset();
    }
 
    /* VcmpxPermlaneHazard
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index d8337418e22..a3f8555e15e 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -1215,12 +1215,12 @@ can_eliminate_and_exec(opt_ctx& ctx, Temp tmp, unsigned pass_flags)
    }
    if (ctx.info[tmp.id()].is_bitwise()) {
       Instruction* instr = ctx.info[tmp.id()].instr;
-      if (instr->pass_flags != pass_flags)
+      if (instr->operands.size() != 2 || instr->pass_flags != pass_flags)
          return false;
-      return std::all_of(
-         instr->operands.begin(), instr->operands.end(),
-         [&](const Operand& op)
-         { return op.isTemp() && can_eliminate_and_exec(ctx, op.getTemp(), pass_flags); });
+      if (!(instr->operands[0].isTemp() && instr->operands[1].isTemp()))
+         return false;
+      return can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), pass_flags) &&
+             can_eliminate_and_exec(ctx, instr->operands[1].getTemp(), pass_flags);
    }
    return false;
 }
diff --git a/src/amd/compiler/aco_optimizer_postRA.cpp b/src/amd/compiler/aco_optimizer_postRA.cpp
index 726cff40a0c..2dda12b3f1c 100644
--- a/src/amd/compiler/aco_optimizer_postRA.cpp
+++ b/src/amd/compiler/aco_optimizer_postRA.cpp
@@ -34,6 +34,9 @@ namespace aco {
 namespace {
 
 constexpr const size_t max_reg_cnt = 512;
+constexpr const size_t max_sgpr_cnt = 128;
+constexpr const size_t min_vgpr = 256;
+constexpr const size_t max_vgpr_cnt = 256;
 
 struct Idx {
    bool operator==(const Idx& other) const { return block == other.block && instr == other.instr; }
@@ -66,17 +69,46 @@ struct pr_opt_ctx {
          std::fill(instr_idx_by_regs[block->index].begin(), instr_idx_by_regs[block->index].end(),
                    not_written_in_block);
       } else {
-         unsigned first_pred = block->linear_preds[0];
-         for (unsigned i = 0; i < max_reg_cnt; i++) {
-            bool all_same = std::all_of(
-               std::next(block->linear_preds.begin()), block->linear_preds.end(),
-               [&](unsigned pred)
-               { return instr_idx_by_regs[pred][i] == instr_idx_by_regs[first_pred][i]; });
+         const uint32_t first_linear_pred = block->linear_preds[0];
+         const std::vector<uint32_t>& linear_preds = block->linear_preds;
+
+         for (unsigned i = 0; i < max_sgpr_cnt; i++) {
+            const bool all_same = std::all_of(
+               std::next(linear_preds.begin()), linear_preds.end(),
+               [=](unsigned pred)
+               { return instr_idx_by_regs[pred][i] == instr_idx_by_regs[first_linear_pred][i]; });
 
             if (all_same)
-               instr_idx_by_regs[block->index][i] = instr_idx_by_regs[first_pred][i];
+               instr_idx_by_regs[block->index][i] = instr_idx_by_regs[first_linear_pred][i];
             else
-               instr_idx_by_regs[block->index][i] = not_written_in_block;
+               instr_idx_by_regs[block->index][i] = written_by_multiple_instrs;
+         }
+
+         if (!block->logical_preds.empty()) {
+            /* We assume that VGPRs are only read by blocks which have a logical predecessor,
+             * ie. any block that reads any VGPR has at least 1 logical predecessor.
+             */
+            const unsigned first_logical_pred = block->logical_preds[0];
+            const std::vector<uint32_t>& logical_preds = block->logical_preds;
+
+            for (unsigned i = min_vgpr; i < (min_vgpr + max_vgpr_cnt); i++) {
+               const bool all_same = std::all_of(
+                  std::next(logical_preds.begin()), logical_preds.end(),
+                  [=](unsigned pred) {
+                     return instr_idx_by_regs[pred][i] == instr_idx_by_regs[first_logical_pred][i];
+                  });
+
+               if (all_same)
+                  instr_idx_by_regs[block->index][i] = instr_idx_by_regs[first_logical_pred][i];
+               else
+                  instr_idx_by_regs[block->index][i] = written_by_multiple_instrs;
+            }
+         } else {
+            /* If a block has no logical predecessors, it is not part of the
+             * logical CFG and therefore it also won't have any logical successors.
+             * Such a block does not write any VGPRs ever.
+             */
+            assert(block->logical_succs.empty());
          }
       }
    }
@@ -127,16 +159,7 @@ last_writer_idx(pr_opt_ctx& ctx, const Operand& op)
    if (op.isConstant() || op.isUndefined())
       return const_or_undef;
 
-   assert(op.physReg().reg() < max_reg_cnt);
-   Idx instr_idx = ctx.instr_idx_by_regs[ctx.current_block->index][op.physReg().reg()];
-
-#ifndef NDEBUG
-   /* Debug mode:  */
-   instr_idx = last_writer_idx(ctx, op.physReg(), op.regClass());
-   assert(instr_idx != written_by_multiple_instrs);
-#endif
-
-   return instr_idx;
+   return last_writer_idx(ctx, op.physReg(), op.regClass());
 }
 
 bool
diff --git a/src/amd/compiler/tests/test_optimizer_postRA.cpp b/src/amd/compiler/tests/test_optimizer_postRA.cpp
index 468a24cdb51..1367af29adf 100644
--- a/src/amd/compiler/tests/test_optimizer_postRA.cpp
+++ b/src/amd/compiler/tests/test_optimizer_postRA.cpp
@@ -369,14 +369,16 @@ BEGIN_TEST(optimizer_postRA.dpp)
 
    /* control flow */
    //! BB1
-   //! /* logical preds: / linear preds: BB0, / kind: uniform, */
+   //! /* logical preds: BB0, / linear preds: BB0, / kind: uniform, */
    //! v1: %res10:v[2] = v_add_f32 %a:v[0], %b:v[1] row_mirror bound_ctrl:1
    //! p_unit_test 10, %res10:v[2]
    Temp tmp10 = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1, reg_v2), a, dpp_row_mirror);
 
    bld.reset(program->create_and_insert_block());
    program->blocks[0].linear_succs.push_back(1);
+   program->blocks[0].logical_succs.push_back(1);
    program->blocks[1].linear_preds.push_back(0);
+   program->blocks[1].logical_preds.push_back(0);
 
    Temp res10 = bld.vop2(aco_opcode::v_add_f32, bld.def(v1, reg_v2), Operand(tmp10, reg_v2), b);
    writeout(10, Operand(res10, reg_v2));
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 4077981fa6d..c8cc7684a37 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -782,7 +782,8 @@ radv_physical_device_try_create(struct radv_instance *instance, drmDevicePtr drm
 
    const char *marketing_name = device->ws->get_chip_name(device->ws);
    snprintf(device->marketing_name, sizeof(device->name), "%s (RADV %s%s)",
-            marketing_name, device->rad_info.name, radv_get_compiler_string(device));
+            marketing_name ? marketing_name : "AMD Unknown", device->rad_info.name,
+            radv_get_compiler_string(device));
 
 #ifdef ENABLE_SHADER_CACHE
    if (radv_device_get_cache_uuid(device, device->cache_uuid)) {
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 52d53e7b1fb..d6a0858dbbc 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -41,7 +41,9 @@
 #include "util/debug.h"
 #include "ac_binary.h"
 #include "ac_nir.h"
+#ifndef _WIN32
 #include "ac_rtld.h"
+#endif
 #include "aco_interface.h"
 #include "sid.h"
 #include "vk_format.h"
@@ -1907,6 +1909,7 @@ radv_postprocess_config(const struct radv_device *device, const struct ac_shader
    }
 }
 
+#ifndef _WIN32
 static bool
 radv_open_rtld_binary(struct radv_device *device, const struct radv_shader *shader,
                       const struct radv_shader_binary *binary, struct ac_rtld_binary *rtld_binary)
@@ -1950,12 +1953,16 @@ radv_open_rtld_binary(struct radv_device *device, const struct radv_shader *shad
 
    return ac_rtld_open(rtld_binary, open_info);
 }
+#endif
 
 bool
 radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_binary *binary,
                           struct radv_shader *shader, void *dest_ptr)
 {
    if (binary->type == RADV_BINARY_TYPE_RTLD) {
+#ifdef _WIN32
+      return false;
+#else
       struct ac_rtld_binary rtld_binary = {0};
 
       if (!radv_open_rtld_binary(device, shader, binary, &rtld_binary)) {
@@ -1977,6 +1984,7 @@ radv_shader_binary_upload(struct radv_device *device, const struct radv_shader_b
 
       shader->code_ptr = dest_ptr;
       ac_rtld_close(&rtld_binary);
+#endif
    } else {
       struct radv_shader_binary_legacy *bin = (struct radv_shader_binary_legacy *)binary;
       memcpy(dest_ptr, bin->data + bin->stats_size, bin->code_size);
@@ -2004,6 +2012,10 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
    shader->ref_count = 1;
 
    if (binary->type == RADV_BINARY_TYPE_RTLD) {
+#ifdef _WIN32
+      free(shader);
+      return NULL;
+#else
       struct ac_rtld_binary rtld_binary = {0};
 
       if (!radv_open_rtld_binary(device, shader, binary, &rtld_binary)) {
@@ -2031,6 +2043,7 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
       shader->code_size = rtld_binary.rx_size;
       shader->exec_size = rtld_binary.exec_size;
       ac_rtld_close(&rtld_binary);
+#endif
    } else {
       assert(binary->type == RADV_BINARY_TYPE_LEGACY);
       config = ((struct radv_shader_binary_legacy *)binary)->base.config;
@@ -2050,6 +2063,10 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
    }
 
    if (binary->type == RADV_BINARY_TYPE_RTLD) {
+#ifdef _WIN32
+      free(shader);
+      return NULL;
+#else
       struct radv_shader_binary_rtld *bin = (struct radv_shader_binary_rtld *)binary;
       struct ac_rtld_binary rtld_binary = {0};
 
@@ -2075,6 +2092,7 @@ radv_shader_create(struct radv_device *device, const struct radv_shader_binary *
          shader->disasm_string[disasm_size] = 0;
       }
       ac_rtld_close(&rtld_binary);
+#endif
    } else {
       struct radv_shader_binary_legacy *bin = (struct radv_shader_binary_legacy *)binary;
 
diff --git a/src/amd/vulkan/si_cmd_buffer.c b/src/amd/vulkan/si_cmd_buffer.c
index fdc8ab5ddd6..11ebd265de2 100644
--- a/src/amd/vulkan/si_cmd_buffer.c
+++ b/src/amd/vulkan/si_cmd_buffer.c
@@ -861,6 +861,12 @@ si_get_ia_multi_vgt_param(struct radv_cmd_buffer *cmd_buffer, bool instanced_dra
       if (gfx_level <= GFX8 && info->max_se == 4 && multi_instances_smaller_than_primgroup)
          wd_switch_on_eop = true;
 
+      /* Hardware requirement when drawing primitives from a stream
+       * output buffer.
+       */
+      if (count_from_stream_output)
+         wd_switch_on_eop = true;
+
       /* Required on GFX7 and later. */
       if (info->max_se > 2 && !wd_switch_on_eop)
          ia_switch_on_eoi = true;
@@ -877,12 +883,6 @@ si_get_ia_multi_vgt_param(struct radv_cmd_buffer *cmd_buffer, bool instanced_dra
       if (family == CHIP_BONAIRE && ia_switch_on_eoi && (instanced_draw || indirect_draw))
          partial_vs_wave = true;
 
-      /* Hardware requirement when drawing primitives from a stream
-       * output buffer.
-       */
-      if (count_from_stream_output)
-         wd_switch_on_eop = true;
-
       /* If the WD switch is false, the IA switch must be false too. */
       assert(wd_switch_on_eop || !ia_switch_on_eop);
    }
diff --git a/src/compiler/nir/nir_lower_task_shader.c b/src/compiler/nir/nir_lower_task_shader.c
index 1417cbdea44..c586145ee62 100644
--- a/src/compiler/nir/nir_lower_task_shader.c
+++ b/src/compiler/nir/nir_lower_task_shader.c
@@ -398,7 +398,8 @@ nir_lower_task_shader(nir_shader *shader,
        * If the shader already had a launch_mesh_workgroups by any chance,
        * this will be removed.
        */
-      builder.cursor = nir_after_cf_list(&builder.impl->body);
+      nir_block *last_block = nir_impl_last_block(impl);
+      builder.cursor = nir_after_block_before_jump(last_block);
       nir_launch_mesh_workgroups(&builder, nir_imm_zero(&builder, 3, 32));
    }
 
diff --git a/src/compiler/nir/nir_lower_to_source_mods.c b/src/compiler/nir/nir_lower_to_source_mods.c
index b44189db5ab..fa84be0f053 100644
--- a/src/compiler/nir/nir_lower_to_source_mods.c
+++ b/src/compiler/nir/nir_lower_to_source_mods.c
@@ -106,7 +106,8 @@ nir_lower_to_source_mods_block(nir_block *block,
             continue;
 
          if (!lower_abs && (parent->op == nir_op_fabs ||
-                            parent->op == nir_op_iabs))
+                            parent->op == nir_op_iabs ||
+                            parent->src[0].abs))
             continue;
 
          nir_instr_rewrite_src(instr, &alu->src[i].src, parent->src[0].src);
diff --git a/src/compiler/nir/nir_xfb_info.h b/src/compiler/nir/nir_xfb_info.h
index 301547f3392..424d730abfb 100644
--- a/src/compiler/nir/nir_xfb_info.h
+++ b/src/compiler/nir/nir_xfb_info.h
@@ -57,7 +57,7 @@ typedef struct nir_xfb_info {
    uint8_t streams_written;
 
    nir_xfb_buffer_info buffers[NIR_MAX_XFB_BUFFERS];
-   uint8_t buffer_to_stream[NIR_MAX_XFB_STREAMS];
+   uint8_t buffer_to_stream[NIR_MAX_XFB_BUFFERS];
 
    uint16_t output_count;
    nir_xfb_output_info outputs[0];
diff --git a/src/drm-shim/drm_shim.c b/src/drm-shim/drm_shim.c
index c7025edda42..672a4e74c9f 100644
--- a/src/drm-shim/drm_shim.c
+++ b/src/drm-shim/drm_shim.c
@@ -52,7 +52,7 @@
 #include "util/u_debug.h"
 #include "drm_shim.h"
 
-#define REAL_FUNCTION_POINTER(x) typeof(x) *real_##x
+#define REAL_FUNCTION_POINTER(x) __typeof__(x) *real_##x
 
 static mtx_t shim_lock = _MTX_INITIALIZER_NP;
 struct set *opendir_set;
diff --git a/src/drm-shim/meson.build b/src/drm-shim/meson.build
index 2a7ea000a61..bd56e642c02 100644
--- a/src/drm-shim/meson.build
+++ b/src/drm-shim/meson.build
@@ -28,7 +28,6 @@ drm_shim = static_library(
   include_directories: [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux],
   dependencies: [dep_libdrm, idep_mesautil, dep_dl],
   gnu_symbol_visibility : 'hidden',
-  override_options : ['c_std=gnu99'],
 )
 dep_drm_shim = declare_dependency(
   link_with: drm_shim,
diff --git a/src/freedreno/ir3/ir3_nir_analyze_ubo_ranges.c b/src/freedreno/ir3/ir3_nir_analyze_ubo_ranges.c
index c6d63d499a8..637995778f7 100644
--- a/src/freedreno/ir3/ir3_nir_analyze_ubo_ranges.c
+++ b/src/freedreno/ir3/ir3_nir_analyze_ubo_ranges.c
@@ -449,16 +449,16 @@ ir3_nir_analyze_ubo_ranges(nir_shader *nir, struct ir3_shader_variant *v)
     * first.
     */
 
-   uint32_t offset = v->num_reserved_user_consts * 16;
+   uint32_t offset = 0;
    for (uint32_t i = 0; i < state->num_enabled; i++) {
       uint32_t range_size = state->range[i].end - state->range[i].start;
 
       assert(offset <= max_upload);
-      state->range[i].offset = offset;
+      state->range[i].offset = offset + v->num_reserved_user_consts * 16;
       assert(offset <= max_upload);
       offset += range_size;
    }
-   state->size = offset - v->num_reserved_user_consts * 16;
+   state->size = offset;
 }
 
 bool
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.c b/src/freedreno/vulkan/tu_cmd_buffer.c
index b6eea06aa07..32a57c4f6e5 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.c
+++ b/src/freedreno/vulkan/tu_cmd_buffer.c
@@ -4273,13 +4273,13 @@ tu6_writes_stencil(struct tu_cmd_buffer *cmd)
       (cmd->state.rb_stencil_cntl & A6XX_RB_STENCIL_CONTROL_ZFAIL_BF__MASK) >> A6XX_RB_STENCIL_CONTROL_ZFAIL_BF__SHIFT;
 
    bool stencil_front_op_writes =
-      front_pass_op != VK_STENCIL_OP_KEEP &&
-      front_fail_op != VK_STENCIL_OP_KEEP &&
+      front_pass_op != VK_STENCIL_OP_KEEP ||
+      front_fail_op != VK_STENCIL_OP_KEEP ||
       front_depth_fail_op != VK_STENCIL_OP_KEEP;
 
    bool stencil_back_op_writes =
-      back_pass_op != VK_STENCIL_OP_KEEP &&
-      back_fail_op != VK_STENCIL_OP_KEEP &&
+      back_pass_op != VK_STENCIL_OP_KEEP ||
+      back_fail_op != VK_STENCIL_OP_KEEP ||
       back_depth_fail_op != VK_STENCIL_OP_KEEP;
 
    return stencil_test_enable &&
diff --git a/src/freedreno/vulkan/tu_device.c b/src/freedreno/vulkan/tu_device.c
index 672881464c7..4b25d559090 100644
--- a/src/freedreno/vulkan/tu_device.c
+++ b/src/freedreno/vulkan/tu_device.c
@@ -1016,7 +1016,7 @@ tu_get_physical_device_properties_1_2(struct tu_physical_device *pdevice,
    p->maxPerStageDescriptorUpdateAfterBindStorageBuffers = max_descriptor_set_size;
    p->maxPerStageDescriptorUpdateAfterBindSampledImages  = max_descriptor_set_size;
    p->maxPerStageDescriptorUpdateAfterBindStorageImages  = max_descriptor_set_size;
-   p->maxPerStageDescriptorUpdateAfterBindInputAttachments = max_descriptor_set_size;
+   p->maxPerStageDescriptorUpdateAfterBindInputAttachments = MAX_RTS;
    p->maxPerStageUpdateAfterBindResources                = max_descriptor_set_size;
    p->maxDescriptorSetUpdateAfterBindSamplers            = max_descriptor_set_size;
    p->maxDescriptorSetUpdateAfterBindUniformBuffers      = max_descriptor_set_size;
@@ -1025,7 +1025,7 @@ tu_get_physical_device_properties_1_2(struct tu_physical_device *pdevice,
    p->maxDescriptorSetUpdateAfterBindStorageBuffersDynamic = MAX_DYNAMIC_STORAGE_BUFFERS;
    p->maxDescriptorSetUpdateAfterBindSampledImages       = max_descriptor_set_size;
    p->maxDescriptorSetUpdateAfterBindStorageImages       = max_descriptor_set_size;
-   p->maxDescriptorSetUpdateAfterBindInputAttachments    = max_descriptor_set_size;
+   p->maxDescriptorSetUpdateAfterBindInputAttachments    = MAX_RTS;
 
    p->supportedDepthResolveModes    = VK_RESOLVE_MODE_SAMPLE_ZERO_BIT;
    p->supportedStencilResolveModes  = VK_RESOLVE_MODE_SAMPLE_ZERO_BIT;
diff --git a/src/freedreno/vulkan/tu_drm.c b/src/freedreno/vulkan/tu_drm.c
index 42e192355cf..9ec4270f4dc 100644
--- a/src/freedreno/vulkan/tu_drm.c
+++ b/src/freedreno/vulkan/tu_drm.c
@@ -487,11 +487,14 @@ tu_timeline_sync_reset(struct vk_device *vk_device,
 static VkResult
 drm_syncobj_wait(struct tu_device *device,
                  uint32_t *handles, uint32_t count_handles,
-                 int64_t timeout_nsec, bool wait_all)
+                 uint64_t timeout_nsec, bool wait_all)
 {
    uint32_t syncobj_wait_flags = DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT;
    if (wait_all) syncobj_wait_flags |= DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL;
 
+   /* syncobj absolute timeouts are signed.  clamp OS_TIMEOUT_INFINITE down. */
+   timeout_nsec = MIN2(timeout_nsec, (uint64_t)INT64_MAX);
+
    int err = drmSyncobjWait(device->fd, handles,
                             count_handles, timeout_nsec,
                             syncobj_wait_flags,
diff --git a/src/gallium/auxiliary/driver_trace/tr_dump_state.c b/src/gallium/auxiliary/driver_trace/tr_dump_state.c
index 45a02a0bd15..9149d4f71cd 100644
--- a/src/gallium/auxiliary/driver_trace/tr_dump_state.c
+++ b/src/gallium/auxiliary/driver_trace/tr_dump_state.c
@@ -149,6 +149,7 @@ void trace_dump_rasterizer_state(const struct pipe_rasterizer_state *state)
 
    trace_dump_member(bool, state, rasterizer_discard);
 
+   trace_dump_member(bool, state, depth_clamp);
    trace_dump_member(bool, state, depth_clip_near);
    trace_dump_member(bool, state, depth_clip_far);
 
diff --git a/src/gallium/auxiliary/meson.build b/src/gallium/auxiliary/meson.build
index 2a2b2a56e50..a9597b002b9 100644
--- a/src/gallium/auxiliary/meson.build
+++ b/src/gallium/auxiliary/meson.build
@@ -474,7 +474,7 @@ endif
 
 if with_glx == 'xlib' or with_glx == 'gallium-xlib'
   files_libgalliumvlwinsys += files('vl/vl_winsys_xlib_swrast.c')
-else
+elif with_gallium_drisw_kms
   files_libgalliumvlwinsys += files('vl/vl_winsys_dri_vgem.c')
 endif
 
diff --git a/src/gallium/auxiliary/pipe-loader/pipe_loader_sw.c b/src/gallium/auxiliary/pipe-loader/pipe_loader_sw.c
index 2cf5c04614d..12bc7930557 100644
--- a/src/gallium/auxiliary/pipe-loader/pipe_loader_sw.c
+++ b/src/gallium/auxiliary/pipe-loader/pipe_loader_sw.c
@@ -404,7 +404,7 @@ pipe_loader_sw_get_driconf(struct pipe_loader_device *dev, unsigned *count)
 
 #if defined(HAVE_DRI) && defined(HAVE_ZINK)
 static const driOptionDescription zink_driconf[] = {
-      #include "zink/driinfo_zink.h"
+      #include "gallium/drivers/zink/driinfo_zink.h"
 };
 
 static const struct driOptionDescription *
diff --git a/src/gallium/auxiliary/util/u_threaded_context.c b/src/gallium/auxiliary/util/u_threaded_context.c
index a26bda9670a..d0ca64682da 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.c
+++ b/src/gallium/auxiliary/util/u_threaded_context.c
@@ -406,18 +406,51 @@ threaded_context_flush(struct pipe_context *_pipe,
    }
 }
 
+/* Must be called before TC binds, maps, invalidates, or adds a buffer to a buffer list. */
+static void tc_touch_buffer(struct threaded_context *tc, struct threaded_resource *buf)
+{
+   const struct threaded_context *first_user = buf->first_user;
+
+   /* Fast path exit to avoid additional branches */
+   if (likely(first_user == tc))
+      return;
+
+   if (!first_user)
+      first_user = p_atomic_cmpxchg_ptr(&buf->first_user, NULL, tc);
+
+   /* The NULL check might seem unnecessary here but it's actually critical:
+    * p_atomic_cmpxchg will return NULL if it succeeds, meaning that NULL is
+    * equivalent to "we're the first user" here. (It's equally important not
+    * to ignore the result of the cmpxchg above, since it might fail.)
+    * Without the NULL check, we'd set the flag unconditionally, which is bad.
+    */
+   if (first_user && first_user != tc && !buf->used_by_multiple_contexts)
+      buf->used_by_multiple_contexts = true;
+}
+
+static bool tc_is_buffer_shared(struct threaded_resource *buf)
+{
+   return buf->is_shared || buf->used_by_multiple_contexts;
+}
+
 static void
-tc_add_to_buffer_list(struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_add_to_buffer_list(struct threaded_context *tc, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
 
 /* Set a buffer binding and add it to the buffer list. */
 static void
-tc_bind_buffer(uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
+tc_bind_buffer(struct threaded_context *tc, uint32_t *binding, struct tc_buffer_list *next, struct pipe_resource *buf)
 {
-   uint32_t id = threaded_resource(buf)->buffer_id_unique;
+   struct threaded_resource *tbuf = threaded_resource(buf);
+   tc_touch_buffer(tc, tbuf);
+
+   uint32_t id = tbuf->buffer_id_unique;
    *binding = id;
    BITSET_SET(next->buffer_list, id & TC_BUFFER_ID_MASK);
 }
@@ -675,6 +708,8 @@ threaded_resource_init(struct pipe_resource *res, bool allow_cpu_storage)
 {
    struct threaded_resource *tres = threaded_resource(res);
 
+   tres->first_user = NULL;
+   tres->used_by_multiple_contexts = false;
    tres->latest = &tres->b;
    tres->cpu_storage = NULL;
    util_range_init(&tres->valid_buffer_range);
@@ -933,7 +968,7 @@ tc_get_query_result_resource(struct pipe_context *_pipe,
    p->result_type = result_type;
    p->index = index;
    tc_set_resource_reference(&p->resource, resource);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->offset = offset;
 }
 
@@ -1225,7 +1260,7 @@ tc_set_constant_buffer(struct pipe_context *_pipe,
       tc_set_resource_reference(&p->cb.buffer, buffer);
 
    if (buffer) {
-      tc_bind_buffer(&tc->const_buffers[shader][index],
+      tc_bind_buffer(tc, &tc->const_buffers[shader][index],
                      &tc->buffer_lists[tc->next_buf_list], buffer);
    } else {
       tc_unbind_buffer(&tc->const_buffers[shader][index]);
@@ -1425,7 +1460,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
 
          for (unsigned i = 0; i < count; i++) {
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1437,7 +1472,7 @@ tc_set_sampler_views(struct pipe_context *_pipe,
             pipe_sampler_view_reference(&p->slot[i], views[i]);
 
             if (views[i] && views[i]->target == PIPE_BUFFER) {
-               tc_bind_buffer(&tc->sampler_buffers[shader][start + i], next,
+               tc_bind_buffer(tc, &tc->sampler_buffers[shader][start + i], next,
                               views[i]->texture);
             } else {
                tc_unbind_buffer(&tc->sampler_buffers[shader][start + i]);
@@ -1516,7 +1551,7 @@ tc_set_shader_images(struct pipe_context *_pipe,
          tc_set_resource_reference(&p->slot[i].resource, resource);
 
          if (resource && resource->target == PIPE_BUFFER) {
-            tc_bind_buffer(&tc->image_buffers[shader][start + i], next, resource);
+            tc_bind_buffer(tc, &tc->image_buffers[shader][start + i], next, resource);
 
             if (images[i].access & PIPE_IMAGE_ACCESS_WRITE) {
                struct threaded_resource *tres = threaded_resource(resource);
@@ -1611,7 +1646,7 @@ tc_set_shader_buffers(struct pipe_context *_pipe,
          if (src->buffer) {
             struct threaded_resource *tres = threaded_resource(src->buffer);
 
-            tc_bind_buffer(&tc->shader_buffers[shader][start + i], next, &tres->b);
+            tc_bind_buffer(tc, &tc->shader_buffers[shader][start + i], next, &tres->b);
 
             if (writable_bitmask & BITFIELD_BIT(i)) {
                tc_buffer_disable_cpu_storage(src->buffer);
@@ -1687,7 +1722,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             struct pipe_resource *buf = buffers[i].buffer.resource;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1705,7 +1740,7 @@ tc_set_vertex_buffers(struct pipe_context *_pipe,
             dst->buffer_offset = src->buffer_offset;
 
             if (buf) {
-               tc_bind_buffer(&tc->vertex_buffers[start + i], next, buf);
+               tc_bind_buffer(tc, &tc->vertex_buffers[start + i], next, buf);
             } else {
                tc_unbind_buffer(&tc->vertex_buffers[start + i]);
             }
@@ -1762,7 +1797,7 @@ tc_set_stream_output_targets(struct pipe_context *_pipe,
       pipe_so_target_reference(&p->targets[i], tgs[i]);
       if (tgs[i]) {
          tc_buffer_disable_cpu_storage(tgs[i]->buffer);
-         tc_bind_buffer(&tc->streamout_buffers[i], next, tgs[i]->buffer);
+         tc_bind_buffer(tc, &tc->streamout_buffers[i], next, tgs[i]->buffer);
       } else {
          tc_unbind_buffer(&tc->streamout_buffers[i]);
       }
@@ -1996,7 +2031,9 @@ tc_call_replace_buffer_storage(struct pipe_context *pipe, void *call, uint64_t *
    return call_size(tc_replace_buffer_storage);
 }
 
-/* Return true if the buffer has been invalidated or is idle. */
+/* Return true if the buffer has been invalidated or is idle.
+ * Note that callers must've called tc_touch_buffer before calling
+ * this function. */
 static bool
 tc_invalidate_buffer(struct threaded_context *tc,
                      struct threaded_resource *tbuf)
@@ -2017,7 +2054,7 @@ tc_invalidate_buffer(struct threaded_context *tc,
    struct pipe_resource *new_buf;
 
    /* Shared, pinned, and sparse buffers can't be reallocated. */
-   if (tbuf->is_shared ||
+   if (tc_is_buffer_shared(tbuf) ||
        tbuf->is_user_ptr ||
        tbuf->b.flags & (PIPE_RESOURCE_FLAG_SPARSE | PIPE_RESOURCE_FLAG_UNMAPPABLE))
       return false;
@@ -2062,6 +2099,8 @@ tc_invalidate_buffer(struct threaded_context *tc,
    return true;
 }
 
+/* Note that callers must've called tc_touch_buffer first before
+ * calling tc_improve_map_buffer_flags. */
 static unsigned
 tc_improve_map_buffer_flags(struct threaded_context *tc,
                             struct threaded_resource *tres, unsigned usage,
@@ -2176,6 +2215,14 @@ tc_buffer_map(struct pipe_context *_pipe,
    if (usage & PIPE_MAP_THREAD_SAFE)
       tc_buffer_disable_cpu_storage(resource);
 
+   tc_touch_buffer(tc, tres);
+
+   /* CPU storage relies on buffer invalidation never failing. With shared buffers,
+    * invalidation might not always be possible, so CPU storage can't be used.
+    */
+   if (tc_is_buffer_shared(tres))
+      tc_buffer_disable_cpu_storage(resource);
+
    usage = tc_improve_map_buffer_flags(tc, tres, usage, box->x, box->width);
 
    /* If the CPU storage is enabled, return it directly. */
@@ -2183,9 +2230,31 @@ tc_buffer_map(struct pipe_context *_pipe,
       /* We can't let resource_copy_region disable the CPU storage. */
       assert(!(tres->b.flags & PIPE_RESOURCE_FLAG_DONT_MAP_DIRECTLY));
 
-      if (!tres->cpu_storage)
+      if (!tres->cpu_storage) {
          tres->cpu_storage = align_malloc(resource->width0, tc->map_buffer_alignment);
 
+         if (tres->cpu_storage && tres->valid_buffer_range.end) {
+            /* The GPU buffer contains valid data. Copy them to the CPU storage. */
+            struct pipe_box box2;
+            struct pipe_transfer *transfer2;
+
+            unsigned valid_range_len = tres->valid_buffer_range.end - tres->valid_buffer_range.start;
+            u_box_1d(tres->valid_buffer_range.start, valid_range_len, &box2);
+
+            tc_sync_msg(tc, "cpu storage GPU -> CPU copy");
+            tc_set_driver_thread(tc);
+
+            void *ret = pipe->buffer_map(pipe, tres->latest ? tres->latest : resource,
+                                         0, PIPE_MAP_READ, &box2, &transfer2);
+            memcpy(&((uint8_t*)tres->cpu_storage)[tres->valid_buffer_range.start],
+                   ret,
+                   valid_range_len);
+            pipe->buffer_unmap(pipe, transfer2);
+
+            tc_clear_driver_thread(tc);
+         }
+      }
+
       if (tres->cpu_storage) {
          struct threaded_transfer *ttrans = slab_zalloc(&tc->pool_transfers);
          ttrans->b.resource = resource;
@@ -2456,7 +2525,10 @@ tc_buffer_unmap(struct pipe_context *_pipe, struct pipe_transfer *transfer)
       assert(tres->cpu_storage);
 
       if (tres->cpu_storage) {
-         tc_invalidate_buffer(tc, tres);
+         /* Invalidations shouldn't fail as long as CPU storage is allowed. */
+         ASSERTED bool invalidated = tc_invalidate_buffer(tc, tres);
+         assert(invalidated);
+
          tc_buffer_subdata(&tc->base, &tres->b,
                            PIPE_MAP_UNSYNCHRONIZED |
                            TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE,
@@ -2570,6 +2642,8 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    if (!size)
       return;
 
+   tc_touch_buffer(tc, tres);
+
    usage |= PIPE_MAP_WRITE;
 
    /* PIPE_MAP_DIRECTLY supresses implicit DISCARD_RANGE. */
@@ -2591,6 +2665,12 @@ tc_buffer_subdata(struct pipe_context *_pipe,
 
       u_box_1d(offset, size, &box);
 
+      /* CPU storage is only useful for partial updates. It can add overhead
+       * on glBufferData calls so avoid using it.
+       */
+      if (!tres->cpu_storage && offset == 0 && size == resource->width0)
+         usage |= TC_TRANSFER_MAP_UPLOAD_CPU_STORAGE;
+
       map = tc_buffer_map(_pipe, resource, 0, usage, &box, &transfer);
       if (map) {
          memcpy(map, data, size);
@@ -2609,7 +2689,7 @@ tc_buffer_subdata(struct pipe_context *_pipe,
    /* This is will always be busy because if it wasn't, tc_improve_map_buffer-
     * _flags would set UNSYNCHRONIZED and we wouldn't get here.
     */
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], resource);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], resource);
    p->usage = usage;
    p->offset = offset;
    p->size = size;
@@ -3239,7 +3319,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
             tc_set_resource_reference(&p->info.index.resource,
                                       info->index.resource);
          }
-         tc_add_to_buffer_list(next, info->index.resource);
+         tc_add_to_buffer_list(tc, next, info->index.resource);
       }
       memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
 
@@ -3251,11 +3331,11 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                                indirect->count_from_stream_output);
 
       if (indirect->buffer)
-         tc_add_to_buffer_list(next, indirect->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->buffer);
       if (indirect->indirect_draw_count)
-         tc_add_to_buffer_list(next, indirect->indirect_draw_count);
+         tc_add_to_buffer_list(tc, next, indirect->indirect_draw_count);
       if (indirect->count_from_stream_output)
-         tc_add_to_buffer_list(next, indirect->count_from_stream_output->buffer);
+         tc_add_to_buffer_list(tc, next, indirect->count_from_stream_output->buffer);
 
       memcpy(&p->indirect, indirect, sizeof(*indirect));
       p->draw.start = draws[0].start;
@@ -3307,7 +3387,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          if (drawid_offset > 0)
             ((struct tc_draw_single_drawid*)p)->drawid_offset = drawid_offset;
@@ -3430,7 +3510,7 @@ tc_draw_vbo(struct pipe_context *_pipe, const struct pipe_draw_info *info,
                tc_set_resource_reference(&p->info.index.resource,
                                          info->index.resource);
             }
-            tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->index.resource);
+            tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->index.resource);
          }
          take_index_buffer_ownership = false;
          memcpy(&p->info, info, DRAW_INFO_SIZE_WITHOUT_MIN_MAX_INDEX);
@@ -3642,7 +3722,7 @@ tc_launch_grid(struct pipe_context *_pipe,
    memcpy(&p->info, info, sizeof(*info));
 
    if (info->indirect)
-      tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], info->indirect);
+      tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], info->indirect);
 
    /* This must be after tc_add_*call, which can flush the batch. */
    if (unlikely(tc->add_all_compute_bindings_to_buffer_list))
@@ -3689,8 +3769,8 @@ tc_resource_copy_region(struct pipe_context *_pipe,
    if (dst->target == PIPE_BUFFER) {
       struct tc_buffer_list *next = &tc->buffer_lists[tc->next_buf_list];
 
-      tc_add_to_buffer_list(next, src);
-      tc_add_to_buffer_list(next, dst);
+      tc_add_to_buffer_list(tc, next, src);
+      tc_add_to_buffer_list(tc, next, dst);
 
       util_range_add(&tdst->b, &tdst->valid_buffer_range,
                      dstx, dstx + src_box->width);
@@ -3826,7 +3906,10 @@ tc_invalidate_resource(struct pipe_context *_pipe,
    struct threaded_context *tc = threaded_context(_pipe);
 
    if (resource->target == PIPE_BUFFER) {
-      tc_invalidate_buffer(tc, threaded_resource(resource));
+      /* This can fail, in which case we simply ignore the invalidation request. */
+      struct threaded_resource *tbuf = threaded_resource(resource);
+      tc_touch_buffer(tc, tbuf);
+      tc_invalidate_buffer(tc, tbuf);
       return;
    }
 
@@ -3994,7 +4077,7 @@ tc_clear_buffer(struct pipe_context *_pipe, struct pipe_resource *res,
    tc_buffer_disable_cpu_storage(res);
 
    tc_set_resource_reference(&p->res, res);
-   tc_add_to_buffer_list(&tc->buffer_lists[tc->next_buf_list], res);
+   tc_add_to_buffer_list(tc, &tc->buffer_lists[tc->next_buf_list], res);
    p->offset = offset;
    p->size = size;
    memcpy(p->clear_value, clear_value, clear_value_size);
diff --git a/src/gallium/auxiliary/util/u_threaded_context.h b/src/gallium/auxiliary/util/u_threaded_context.h
index 848c56abb89..ce63e74393f 100644
--- a/src/gallium/auxiliary/util/u_threaded_context.h
+++ b/src/gallium/auxiliary/util/u_threaded_context.h
@@ -315,6 +315,17 @@ typedef bool (*tc_is_resource_busy)(struct pipe_screen *screen,
 struct threaded_resource {
    struct pipe_resource b;
 
+   /* Pointer to the TC that first used this threaded_resource (buffer). This is used to
+    * allow TCs to determine whether they have been given a buffer that was created by a
+    * different TC, in which case all TCs have to disable busyness tracking and buffer
+    * replacement for that particular buffer.
+    * DO NOT DEREFERENCE. The only operation allowed on this pointer is equality-checking
+    * since it might be dangling if a buffer has been shared and its first_user has
+    * already been destroyed. The pointer is const void to discourage such disallowed usage.
+    * This is NULL if no TC has used this buffer yet.
+    */
+   const void *first_user;
+
    /* Since buffer invalidations are queued, we can't use the base resource
     * for unsychronized mappings. This points to the latest version of
     * the buffer after the latest invalidation. It's only used for unsychro-
@@ -342,6 +353,12 @@ struct threaded_resource {
     */
    struct util_range valid_buffer_range;
 
+   /* True if multiple threaded contexts have accessed this buffer.
+    * Disables non-multicontext-safe optimizations in TC.
+    * We can't just re-use is_shared for that purpose as that would confuse drivers.
+    */
+   bool used_by_multiple_contexts;
+
    /* Drivers are required to update this for shared resources and user
     * pointers. */
    bool is_shared;
diff --git a/src/gallium/drivers/iris/iris_batch.c b/src/gallium/drivers/iris/iris_batch.c
index 6170fe94fef..0c84485eafa 100644
--- a/src/gallium/drivers/iris/iris_batch.c
+++ b/src/gallium/drivers/iris/iris_batch.c
@@ -487,7 +487,7 @@ create_batch(struct iris_batch *batch)
 
    /* TODO: We probably could suballocate batches... */
    batch->bo = iris_bo_alloc(bufmgr, "command buffer",
-                             BATCH_SZ + BATCH_RESERVED, 1,
+                             BATCH_SZ + BATCH_RESERVED, 8,
                              IRIS_MEMZONE_OTHER, BO_ALLOC_NO_SUBALLOC);
    iris_get_backing_bo(batch->bo)->real.kflags |= EXEC_OBJECT_CAPTURE;
    batch->map = iris_bo_map(NULL, batch->bo, MAP_READ | MAP_WRITE);
diff --git a/src/gallium/drivers/iris/iris_binder.c b/src/gallium/drivers/iris/iris_binder.c
index d2ae1f9092d..ef562e719c0 100644
--- a/src/gallium/drivers/iris/iris_binder.c
+++ b/src/gallium/drivers/iris/iris_binder.c
@@ -69,7 +69,7 @@ binder_realloc(struct iris_context *ice)
    if (binder->bo)
       iris_bo_unreference(binder->bo);
 
-   binder->bo = iris_bo_alloc(bufmgr, "binder", binder->size, 1,
+   binder->bo = iris_bo_alloc(bufmgr, "binder", binder->size, binder->alignment,
                               IRIS_MEMZONE_BINDER, 4096);
    binder->map = iris_bo_map(NULL, binder->bo, MAP_WRITE);
 
diff --git a/src/gallium/drivers/iris/iris_blit.c b/src/gallium/drivers/iris/iris_blit.c
index 3b32d65eeb1..0b505268ff1 100644
--- a/src/gallium/drivers/iris/iris_blit.c
+++ b/src/gallium/drivers/iris/iris_blit.c
@@ -684,13 +684,13 @@ iris_copy_region(struct blorp_context *blorp,
 
    if (dst->target == PIPE_BUFFER && src->target == PIPE_BUFFER) {
       struct blorp_address src_addr = {
-         .buffer = src_res->bo, .offset = src_box->x,
+         .buffer = src_res->bo, .offset = src_res->offset + src_box->x,
          .mocs = iris_mocs(src_res->bo, &screen->isl_dev,
                            ISL_SURF_USAGE_TEXTURE_BIT),
          .local_hint = iris_bo_likely_local(src_res->bo),
       };
       struct blorp_address dst_addr = {
-         .buffer = dst_res->bo, .offset = dstx,
+         .buffer = dst_res->bo, .offset = dst_res->offset + dstx,
          .reloc_flags = EXEC_OBJECT_WRITE,
          .mocs = iris_mocs(dst_res->bo, &screen->isl_dev,
                            ISL_SURF_USAGE_RENDER_TARGET_BIT),
diff --git a/src/gallium/drivers/iris/iris_border_color.c b/src/gallium/drivers/iris/iris_border_color.c
index e7f1015c60c..fdc46912099 100644
--- a/src/gallium/drivers/iris/iris_border_color.c
+++ b/src/gallium/drivers/iris/iris_border_color.c
@@ -75,7 +75,7 @@ iris_init_border_color_pool(struct iris_bufmgr *bufmgr,
    pool->ht = _mesa_hash_table_create(NULL, color_hash, color_equals);
 
    pool->bo = iris_bo_alloc(bufmgr, "border colors",
-                            IRIS_BORDER_COLOR_POOL_SIZE, 1,
+                            IRIS_BORDER_COLOR_POOL_SIZE, 64,
                             IRIS_MEMZONE_BORDER_COLOR_POOL, 0);
    pool->map = iris_bo_map(NULL, pool->bo, MAP_WRITE);
 
diff --git a/src/gallium/drivers/iris/iris_measure.c b/src/gallium/drivers/iris/iris_measure.c
index 0d93ebd91b3..a27758c5e9f 100644
--- a/src/gallium/drivers/iris/iris_measure.c
+++ b/src/gallium/drivers/iris/iris_measure.c
@@ -118,7 +118,7 @@ iris_init_batch_measure(struct iris_context *ice, struct iris_batch *batch)
    struct iris_measure_batch *measure = batch->measure;
 
    measure->bo = iris_bo_alloc(bufmgr, "measure",
-                               config->batch_size * sizeof(uint64_t), 1,
+                               config->batch_size * sizeof(uint64_t), 8,
                                IRIS_MEMZONE_OTHER, BO_ALLOC_ZEROED);
    measure->base.timestamps = iris_bo_map(NULL, measure->bo, MAP_READ);
    measure->base.framebuffer =
diff --git a/src/gallium/drivers/iris/iris_perf.c b/src/gallium/drivers/iris/iris_perf.c
index 3bf2dbcbd51..407030d613f 100644
--- a/src/gallium/drivers/iris/iris_perf.c
+++ b/src/gallium/drivers/iris/iris_perf.c
@@ -26,7 +26,7 @@
 static void *
 iris_oa_bo_alloc(void *bufmgr, const char *name, uint64_t size)
 {
-   return iris_bo_alloc(bufmgr, name, size, 1, IRIS_MEMZONE_OTHER, BO_ALLOC_SMEM);
+   return iris_bo_alloc(bufmgr, name, size, 64, IRIS_MEMZONE_OTHER, BO_ALLOC_SMEM);
 }
 
 static void
diff --git a/src/gallium/drivers/iris/iris_program.c b/src/gallium/drivers/iris/iris_program.c
index f4898ae897f..bb8d1576c29 100644
--- a/src/gallium/drivers/iris/iris_program.c
+++ b/src/gallium/drivers/iris/iris_program.c
@@ -2389,7 +2389,8 @@ iris_get_scratch_space(struct iris_context *ice,
    if (!*bop) {
       assert(stage < ARRAY_SIZE(devinfo->max_scratch_ids));
       uint32_t size = per_thread_scratch * devinfo->max_scratch_ids[stage];
-      *bop = iris_bo_alloc(bufmgr, "scratch", size, 1, IRIS_MEMZONE_SHADER, 0);
+      *bop = iris_bo_alloc(bufmgr, "scratch", size, 1024,
+                           IRIS_MEMZONE_SHADER, 0);
    }
 
    return *bop;
diff --git a/src/gallium/drivers/iris/iris_resource.c b/src/gallium/drivers/iris/iris_resource.c
index c432788421d..f259f329e7c 100644
--- a/src/gallium/drivers/iris/iris_resource.c
+++ b/src/gallium/drivers/iris/iris_resource.c
@@ -1076,6 +1076,22 @@ iris_resource_finish_aux_import(struct pipe_screen *pscreen,
    }
 }
 
+static uint32_t
+iris_buffer_alignment(uint64_t size)
+{
+   /* Some buffer operations want some amount of alignment.  The largest
+    * buffer texture pixel size is 4 * 4 = 16B.  OpenCL data is also supposed
+    * to be aligned and largest OpenCL data type is a double16 which is
+    * 8 * 16 = 128B.  Align to the largest power of 2 which fits in the size,
+    * up to 128B.
+    */
+   uint32_t align = MAX2(4 * 4, 8 * 16);
+   while (align > size)
+      align >>= 1;
+
+   return align;
+}
+
 static struct pipe_resource *
 iris_resource_create_for_buffer(struct pipe_screen *pscreen,
                                 const struct pipe_resource *templ)
@@ -1110,8 +1126,9 @@ iris_resource_create_for_buffer(struct pipe_screen *pscreen,
 
    unsigned flags = iris_resource_alloc_flags(screen, templ, res->aux.usage);
 
-   res->bo =
-      iris_bo_alloc(screen->bufmgr, name, templ->width0, 1, memzone, flags);
+   res->bo = iris_bo_alloc(screen->bufmgr, name, templ->width0,
+                           iris_buffer_alignment(templ->width0),
+                           memzone, flags);
 
    if (!res->bo) {
       iris_resource_destroy(pscreen, &res->base.b);
@@ -1913,7 +1930,8 @@ iris_invalidate_resource(struct pipe_context *ctx,
 
    struct iris_bo *old_bo = res->bo;
    struct iris_bo *new_bo =
-      iris_bo_alloc(screen->bufmgr, res->bo->name, resource->width0, 1,
+      iris_bo_alloc(screen->bufmgr, res->bo->name, resource->width0,
+                    iris_buffer_alignment(resource->width0),
                     iris_memzone_for_address(old_bo->address), 0);
    if (!new_bo)
       return;
diff --git a/src/gallium/drivers/iris/iris_screen.c b/src/gallium/drivers/iris/iris_screen.c
index bc4878e9168..dd89acb4732 100644
--- a/src/gallium/drivers/iris/iris_screen.c
+++ b/src/gallium/drivers/iris/iris_screen.c
@@ -773,6 +773,19 @@ iris_init_identifier_bo(struct iris_screen *screen)
 struct pipe_screen *
 iris_screen_create(int fd, const struct pipe_screen_config *config)
 {
+   struct iris_screen *screen = rzalloc(NULL, struct iris_screen);
+   if (!screen)
+      return NULL;
+
+   if (!intel_get_device_info_from_fd(fd, &screen->devinfo))
+      return NULL;
+   screen->pci_id = screen->devinfo.pci_device_id;
+
+   p_atomic_set(&screen->refcount, 1);
+
+   if (screen->devinfo.ver < 8 || screen->devinfo.platform == INTEL_PLATFORM_CHV)
+      return NULL;
+
    /* Here are the i915 features we need for Iris (in chronological order) :
     *    - I915_PARAM_HAS_EXEC_NO_RELOC     (3.10)
     *    - I915_PARAM_HAS_EXEC_HANDLE_LUT   (3.10)
@@ -787,19 +800,6 @@ iris_screen_create(int fd, const struct pipe_screen_config *config)
       return NULL;
    }
 
-   struct iris_screen *screen = rzalloc(NULL, struct iris_screen);
-   if (!screen)
-      return NULL;
-
-   if (!intel_get_device_info_from_fd(fd, &screen->devinfo))
-      return NULL;
-   screen->pci_id = screen->devinfo.pci_device_id;
-
-   p_atomic_set(&screen->refcount, 1);
-
-   if (screen->devinfo.ver < 8 || screen->devinfo.platform == INTEL_PLATFORM_CHV)
-      return NULL;
-
    driParseConfigFiles(config->options, config->options_info, 0, "iris",
                        NULL, NULL, NULL, 0, NULL, 0);
 
@@ -825,7 +825,7 @@ iris_screen_create(int fd, const struct pipe_screen_config *config)
    screen->id = iris_bufmgr_create_screen_id(screen->bufmgr);
 
    screen->workaround_bo =
-      iris_bo_alloc(screen->bufmgr, "workaround", 4096, 1,
+      iris_bo_alloc(screen->bufmgr, "workaround", 4096, 4096,
                     IRIS_MEMZONE_OTHER, BO_ALLOC_NO_SUBALLOC);
    if (!screen->workaround_bo)
       return NULL;
diff --git a/src/gallium/drivers/llvmpipe/lp_state_cs.c b/src/gallium/drivers/llvmpipe/lp_state_cs.c
index 273dd8b944b..5b26bf7265f 100644
--- a/src/gallium/drivers/llvmpipe/lp_state_cs.c
+++ b/src/gallium/drivers/llvmpipe/lp_state_cs.c
@@ -634,7 +634,7 @@ make_variant_key(struct llvmpipe_context *lp,
           * This will still work, the only downside is that not actually
           * used views may be included in the shader key.
           */
-         if(shader->info.base.file_mask[TGSI_FILE_SAMPLER_VIEW] & (1u << (i & 31))) {
+         if((shader->info.base.file_mask[TGSI_FILE_SAMPLER_VIEW] & (1u << (i & 31))) || i > 31) {
             lp_sampler_static_texture_state(&cs_sampler[i].texture_state,
                                             lp->sampler_views[PIPE_SHADER_COMPUTE][i]);
          }
@@ -643,7 +643,7 @@ make_variant_key(struct llvmpipe_context *lp,
    else {
       key->nr_sampler_views = key->nr_samplers;
       for(i = 0; i < key->nr_sampler_views; ++i) {
-         if(shader->info.base.file_mask[TGSI_FILE_SAMPLER] & (1 << i)) {
+         if((shader->info.base.file_mask[TGSI_FILE_SAMPLER] & (1 << i)) || i > 31) {
             lp_sampler_static_texture_state(&cs_sampler[i].texture_state,
                                             lp->sampler_views[PIPE_SHADER_COMPUTE][i]);
          }
@@ -653,8 +653,11 @@ make_variant_key(struct llvmpipe_context *lp,
    struct lp_image_static_state *lp_image;
    lp_image = lp_cs_variant_key_images(key);
    key->nr_images = shader->info.base.file_max[TGSI_FILE_IMAGE] + 1;
+   if (key->nr_images)
+      memset(lp_image, 0,
+             key->nr_images * sizeof *lp_image);
    for (i = 0; i < key->nr_images; ++i) {
-      if (shader->info.base.file_mask[TGSI_FILE_IMAGE] & (1 << i)) {
+      if ((shader->info.base.file_mask[TGSI_FILE_IMAGE] & (1 << i)) || i > 31) {
          lp_sampler_static_texture_state_image(&lp_image[i].image_state,
                                                &lp->images[PIPE_SHADER_COMPUTE][i]);
       }
diff --git a/src/gallium/drivers/llvmpipe/lp_state_fs.c b/src/gallium/drivers/llvmpipe/lp_state_fs.c
index 658642664ee..14318687e28 100644
--- a/src/gallium/drivers/llvmpipe/lp_state_fs.c
+++ b/src/gallium/drivers/llvmpipe/lp_state_fs.c
@@ -4466,8 +4466,8 @@ make_variant_key(struct llvmpipe_context *lp,
           * This will still work, the only downside is that not actually
           * used views may be included in the shader key.
           */
-         if (shader->info.base.file_mask[TGSI_FILE_SAMPLER_VIEW]
-             & (1u << (i & 31))) {
+         if ((shader->info.base.file_mask[TGSI_FILE_SAMPLER_VIEW]
+              & (1u << (i & 31))) || i > 31) {
             lp_sampler_static_texture_state(&fs_sampler[i].texture_state,
                                   lp->sampler_views[PIPE_SHADER_FRAGMENT][i]);
          }
@@ -4476,7 +4476,7 @@ make_variant_key(struct llvmpipe_context *lp,
    else {
       key->nr_sampler_views = key->nr_samplers;
       for (unsigned i = 0; i < key->nr_sampler_views; ++i) {
-         if (shader->info.base.file_mask[TGSI_FILE_SAMPLER] & (1 << i)) {
+         if ((shader->info.base.file_mask[TGSI_FILE_SAMPLER] & (1 << i)) || i > 31) {
             lp_sampler_static_texture_state(&fs_sampler[i].texture_state,
                                  lp->sampler_views[PIPE_SHADER_FRAGMENT][i]);
          }
@@ -4485,8 +4485,12 @@ make_variant_key(struct llvmpipe_context *lp,
 
    struct lp_image_static_state *lp_image = lp_fs_variant_key_images(key);
    key->nr_images = shader->info.base.file_max[TGSI_FILE_IMAGE] + 1;
+
+   if (key->nr_images)
+      memset(lp_image, 0,
+             key->nr_images * sizeof *lp_image);
    for (unsigned i = 0; i < key->nr_images; ++i) {
-      if (shader->info.base.file_mask[TGSI_FILE_IMAGE] & (1 << i)) {
+      if ((shader->info.base.file_mask[TGSI_FILE_IMAGE] & (1 << i)) || i > 31) {
          lp_sampler_static_texture_state_image(&lp_image[i].image_state,
                                       &lp->images[PIPE_SHADER_FRAGMENT][i]);
       }
diff --git a/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp b/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
index 8a2370fde35..aca89cefeaf 100644
--- a/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
@@ -1223,6 +1223,9 @@ bool AluInstr::from_nir(nir_alu_instr *alu, Shader& shader)
       case nir_op_umul_high: return emit_alu_trans_op2_cayman(*alu, op2_mulhi_uint, shader);
       case nir_op_f2u32: return emit_alu_op1(*alu, op1_flt_to_uint, shader);
       case nir_op_f2i32: return emit_alu_op1(*alu, op1_flt_to_int, shader);
+      case nir_op_ishl: return emit_alu_op2_int(*alu, op2_lshl_int, shader);
+      case nir_op_ishr: return emit_alu_op2_int(*alu, op2_ashr_int, shader);
+      case nir_op_ushr: return emit_alu_op2_int(*alu, op2_lshr_int, shader);
       default:
          ;
       }
diff --git a/src/gallium/drivers/radeonsi/si_fence.c b/src/gallium/drivers/radeonsi/si_fence.c
index b698af0cd7c..ddd799ccd11 100644
--- a/src/gallium/drivers/radeonsi/si_fence.c
+++ b/src/gallium/drivers/radeonsi/si_fence.c
@@ -571,8 +571,11 @@ static void si_fence_server_signal(struct pipe_context *ctx, struct pipe_fence_h
     * operation.
     *
     * Forces a flush even if the GFX CS is empty.
+    *
+    * The flush must not be asynchronous because the kernel must receive
+    * the scheduled "signal" operation before any wait.
     */
-   si_flush_all_queues(ctx, NULL, PIPE_FLUSH_ASYNC, true);
+   si_flush_all_queues(ctx, NULL, 0, true);
 }
 
 static void si_fence_server_sync(struct pipe_context *ctx, struct pipe_fence_handle *fence)
diff --git a/src/gallium/drivers/radeonsi/si_state_draw.cpp b/src/gallium/drivers/radeonsi/si_state_draw.cpp
index bace555b996..ee42f33130a 100644
--- a/src/gallium/drivers/radeonsi/si_state_draw.cpp
+++ b/src/gallium/drivers/radeonsi/si_state_draw.cpp
@@ -1320,7 +1320,7 @@ static void gfx10_emit_ge_cntl(struct si_context *sctx, unsigned num_patches)
                G_03096C_PRIM_GRP_SIZE_GFX11(si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->current->ge_cntl);
 
             ge_cntl = S_03096C_PRIMS_PER_SUBGRP(num_patches) |
-                      S_03096C_VERTS_PER_SUBGRP(0) |
+                      S_03096C_VERTS_PER_SUBGRP(si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->current->ngg.hw_max_esverts) |
                       S_03096C_BREAK_PRIMGRP_AT_EOI(key.u.tess_uses_prim_id) |
                       S_03096C_PRIM_GRP_SIZE_GFX11(prim_grp_size);
          } else {
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
index aa2091a2fcf..d3a777aa85c 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
@@ -1614,7 +1614,7 @@ emit_so_outputs(struct ntv_context *ctx,
                for (unsigned c = 0; c < so_output.num_components; c++) {
                   components[c] = so_output.start_component + c;
                   /* this is the second half of a 2 * vec4 array */
-                  if (slot == VARYING_SLOT_CLIP_DIST1)
+                  if (slot == VARYING_SLOT_CLIP_DIST1 || slot == VARYING_SLOT_CULL_DIST1)
                      components[c] += 4;
                }
                /* OpVectorShuffle can select vector members into a differently-sized vector */
@@ -1645,7 +1645,7 @@ emit_so_outputs(struct ntv_context *ctx,
                 uint32_t member = so_output.start_component + c;
                 SpvId base_type = get_glsl_basetype(ctx, glsl_get_base_type(bare_type));
 
-                if (slot == VARYING_SLOT_CLIP_DIST1)
+                if (slot == VARYING_SLOT_CLIP_DIST1 || slot == VARYING_SLOT_CULL_DIST1)
                    member += 4;
                 components[idx] = spirv_builder_emit_composite_extract(&ctx->builder, base_type, src, &member, 1);
                 if (glsl_type_is_64bit(bare_type)) {
@@ -1901,19 +1901,6 @@ needs_derivative_control(nir_alu_instr *alu)
    }
 }
 
-static SpvId
-unswizzle_src(struct ntv_context *ctx, nir_ssa_def *ssa, SpvId src, unsigned num_components)
-{
-   /* value may have already been cast to ivec, so cast back */
-   SpvId cast_type = get_uvec_type(ctx, ssa->bit_size, num_components);
-   src = emit_bitcast(ctx, cast_type, src);
-
-   /* extract from swizzled vec */
-   SpvId type = spirv_builder_type_uint(&ctx->builder, ssa->bit_size);
-   uint32_t idx = 0;
-   return spirv_builder_emit_composite_extract(&ctx->builder, type, src, &idx, 1);
-}
-
 static void
 emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
 {
@@ -1930,34 +1917,6 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
    if (needs_derivative_control(alu))
       spirv_builder_emit_cap(&ctx->builder, SpvCapabilityDerivativeControl);
 
-   /* modify params here */
-   switch (alu->op) {
-   /* Offset must be an integer type scalar.
-    * Offset is the lowest-order bit of the bit field.
-    * It is consumed as an unsigned value.
-    *
-    * Count must be an integer type scalar.
-    *
-    * if these ops have more than one component in the dest, then their offset and count
-    * are swizzled like ssa_1.xxx, but only a single scalar can be provided
-    */
-   case nir_op_ubitfield_extract:
-   case nir_op_ibitfield_extract:
-      if (num_components > 1) {
-         src[1] = unswizzle_src(ctx, alu->src[1].src.ssa, src[1], num_components);
-         src[2] = unswizzle_src(ctx, alu->src[2].src.ssa, src[2], num_components);
-      }
-      break;
-   case nir_op_bitfield_insert:
-      if (num_components > 1) {
-         src[2] = unswizzle_src(ctx, alu->src[2].src.ssa, src[2], num_components);
-         src[3] = unswizzle_src(ctx, alu->src[3].src.ssa, src[3], num_components);
-      }
-      break;
-   default:
-      break;
-   }
-
    SpvId result = 0;
    switch (alu->op) {
    case nir_op_mov:
@@ -2067,9 +2026,13 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
       result = emit_builtin_unop(ctx, GLSLstd450PackHalf2x16, get_dest_type(ctx, &alu->dest.dest, nir_type_uint), src[0]);
       break;
 
+   case nir_op_unpack_64_2x32:
+      assert(nir_op_infos[alu->op].num_inputs == 1);
+      result = emit_builtin_unop(ctx, GLSLstd450UnpackDouble2x32, get_dest_type(ctx, &alu->dest.dest, nir_type_uint), src[0]);
+      break;
+
    BUILTIN_UNOPF(nir_op_unpack_half_2x16, GLSLstd450UnpackHalf2x16)
    BUILTIN_UNOPF(nir_op_pack_64_2x32, GLSLstd450PackDouble2x32)
-   BUILTIN_UNOPF(nir_op_unpack_64_2x32, GLSLstd450UnpackDouble2x32)
 #undef BUILTIN_UNOP
 #undef BUILTIN_UNOPF
 
diff --git a/src/gallium/drivers/zink/zink_blit.c b/src/gallium/drivers/zink/zink_blit.c
index 3cfd51dd50d..ee41ed6244a 100644
--- a/src/gallium/drivers/zink/zink_blit.c
+++ b/src/gallium/drivers/zink/zink_blit.c
@@ -31,9 +31,12 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info, bool *
        info->alpha_blend)
       return false;
 
-   if (info->src.box.width != info->dst.box.width ||
-       info->src.box.height != info->dst.box.height ||
-       info->src.box.depth != info->dst.box.depth)
+   if (info->src.box.width < 0 ||
+       info->dst.box.width < 0 ||
+       info->src.box.height < 0 ||
+       info->dst.box.height < 0 ||
+       info->src.box.depth < 0 ||
+       info->dst.box.depth < 0)
       return false;
 
    if (info->render_condition_enable &&
diff --git a/src/gallium/drivers/zink/zink_clear.c b/src/gallium/drivers/zink/zink_clear.c
index bfe812f2cd4..e56126ce1ed 100644
--- a/src/gallium/drivers/zink/zink_clear.c
+++ b/src/gallium/drivers/zink/zink_clear.c
@@ -118,23 +118,25 @@ clear_in_rp(struct pipe_context *pctx,
       ctx->base.texture_barrier(&ctx->base, PIPE_TEXTURE_BARRIER_FRAMEBUFFER);
 }
 
+static struct zink_framebuffer_clear_data *
+add_new_clear(struct zink_framebuffer_clear *fb_clear)
+{
+   struct zink_framebuffer_clear_data cd = {0};
+   util_dynarray_append(&fb_clear->clears, struct zink_framebuffer_clear_data, cd);
+   return zink_fb_clear_element(fb_clear, zink_fb_clear_count(fb_clear) - 1);
+}
+
 static struct zink_framebuffer_clear_data *
 get_clear_data(struct zink_context *ctx, struct zink_framebuffer_clear *fb_clear, const struct pipe_scissor_state *scissor_state)
 {
-   struct zink_framebuffer_clear_data *clear = NULL;
    unsigned num_clears = zink_fb_clear_count(fb_clear);
    if (num_clears) {
       struct zink_framebuffer_clear_data *last_clear = zink_fb_clear_element(fb_clear, num_clears - 1);
       /* if we're completely overwriting the previous clear, merge this into the previous clear */
       if (!scissor_state || (last_clear->has_scissor && scissor_states_equal(&last_clear->scissor, scissor_state)))
-         clear = last_clear;
-   }
-   if (!clear) {
-      struct zink_framebuffer_clear_data cd = {0};
-      util_dynarray_append(&fb_clear->clears, struct zink_framebuffer_clear_data, cd);
-      clear = zink_fb_clear_element(fb_clear, zink_fb_clear_count(fb_clear) - 1);
+         return last_clear;
    }
-   return clear;
+   return add_new_clear(fb_clear);
 }
 
 static void
@@ -247,7 +249,30 @@ zink_clear(struct pipe_context *pctx,
       union pipe_color_union color;
       color.f[0] = color.f[1] = color.f[2] = 0;
       color.f[3] = 1.0;
-      pctx->clear(pctx, void_clears, NULL, &color, 0, 0);
+      for (unsigned i = 0; i < fb->nr_cbufs; i++) {
+         if ((void_clears & (PIPE_CLEAR_COLOR0 << i)) && fb->cbufs[i]) {
+            struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+            unsigned num_clears = zink_fb_clear_count(fb_clear);
+            if (num_clears) {
+               if (zink_fb_clear_first_needs_explicit(fb_clear)) {
+                  /* a scissored clear exists:
+                   * - extend the clear array
+                   * - shift existing clears back by one position
+                   * - inject void clear base of array
+                   */
+                  add_new_clear(fb_clear);
+                  struct zink_framebuffer_clear_data *clear = fb_clear->clears.data;
+                  memcpy(clear + 1, clear, num_clears);
+                  memcpy(&clear->color, &color, sizeof(color));
+               } else {
+                  /* no void clear needed */
+               }
+               void_clears &= ~(PIPE_CLEAR_COLOR0 << i);
+            }
+         }
+      }
+      if (void_clears)
+         pctx->clear(pctx, void_clears, NULL, &color, 0, 0);
    }
 
    if (buffers & PIPE_CLEAR_COLOR) {
diff --git a/src/gallium/drivers/zink/zink_compiler.c b/src/gallium/drivers/zink/zink_compiler.c
index fb641c14b54..a86ead30335 100644
--- a/src/gallium/drivers/zink/zink_compiler.c
+++ b/src/gallium/drivers/zink/zink_compiler.c
@@ -21,6 +21,7 @@
  * USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
 
+#include "nir_opcodes.h"
 #include "zink_context.h"
 #include "zink_compiler.h"
 #include "zink_program.h"
@@ -833,7 +834,6 @@ update_so_info(struct zink_shader *zs, const struct pipe_stream_output_info *so_
 
    bool have_fake_psiz = false;
    nir_foreach_shader_out_variable(var, zs->nir) {
-      var->data.explicit_xfb_buffer = 0;
       if (var->data.location == VARYING_SLOT_PSIZ && !var->data.explicit_location)
          have_fake_psiz = true;
    }
@@ -1051,6 +1051,7 @@ rewrite_bo_access_instr(nir_builder *b, nir_instr *instr, void *data)
    nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
    b->cursor = nir_before_instr(instr);
    switch (intr->intrinsic) {
+   case nir_intrinsic_ssbo_atomic_fadd:
    case nir_intrinsic_ssbo_atomic_add:
    case nir_intrinsic_ssbo_atomic_umin:
    case nir_intrinsic_ssbo_atomic_imin:
@@ -1061,8 +1062,8 @@ rewrite_bo_access_instr(nir_builder *b, nir_instr *instr, void *data)
    case nir_intrinsic_ssbo_atomic_xor:
    case nir_intrinsic_ssbo_atomic_exchange:
    case nir_intrinsic_ssbo_atomic_comp_swap: {
-      /* convert offset to uint[idx] */
-      nir_ssa_def *offset = nir_udiv_imm(b, intr->src[1].ssa, 4);
+      /* convert offset to uintN_t[idx] */
+      nir_ssa_def *offset = nir_udiv_imm(b, intr->src[1].ssa, nir_dest_bit_size(intr->dest) / 8);
       nir_instr_rewrite_src_ssa(instr, &intr->src[1], offset);
       return true;
    }
@@ -1251,11 +1252,9 @@ rewrite_atomic_ssbo_instr(nir_builder *b, nir_instr *instr, struct bo_vars *bo)
    default:
       unreachable("unknown intrinsic");
    }
-   /* atomic ops are always 32bit */
-   assert(nir_dest_bit_size(intr->dest) == 32);
    nir_ssa_def *offset = intr->src[1].ssa;
    nir_src *src = &intr->src[0];
-   nir_variable *var = get_bo_var(b->shader, bo, true, src, 32);
+   nir_variable *var = get_bo_var(b->shader, bo, true, src, nir_dest_bit_size(intr->dest));
    nir_deref_instr *deref_var = nir_build_deref_var(b, var);
    nir_ssa_def *idx = src->ssa;
    if (bo->first_ssbo)
@@ -1269,7 +1268,7 @@ rewrite_atomic_ssbo_instr(nir_builder *b, nir_instr *instr, struct bo_vars *bo)
    for (unsigned i = 0; i < num_components; i++) {
       nir_deref_instr *deref_arr = nir_build_deref_array(b, deref_struct, offset);
       nir_intrinsic_instr *new_instr = nir_intrinsic_instr_create(b->shader, op);
-      nir_ssa_dest_init(&new_instr->instr, &new_instr->dest, 1, 32, "");
+      nir_ssa_dest_init(&new_instr->instr, &new_instr->dest, 1, nir_dest_bit_size(intr->dest), "");
       new_instr->src[0] = nir_src_for_ssa(&deref_arr->dest.ssa);
       /* deref ops have no offset src, so copy the srcs after it */
       for (unsigned i = 2; i < nir_intrinsic_infos[intr->intrinsic].num_srcs; i++)
@@ -1528,7 +1527,7 @@ rewrite_64bit_type(nir_shader *nir, const struct glsl_type *type, nir_variable *
 {
    if (glsl_type_is_array(type)) {
       const struct glsl_type *child = glsl_get_array_element(type);
-      unsigned elements = glsl_get_aoa_size(type);
+      unsigned elements = glsl_array_size(type);
       unsigned stride = glsl_get_explicit_stride(type);
       return glsl_array_type(rewrite_64bit_type(nir, child, var), elements, stride);
    }
@@ -1835,6 +1834,7 @@ lower_64bit_vars(nir_shader *shader)
    if (progress) {
       nir_lower_alu_to_scalar(shader, filter_64_bit_instr, NULL);
       nir_lower_phis_to_scalar(shader, false);
+      optimize_nir(shader, NULL);
    }
    return progress;
 }
@@ -2336,6 +2336,7 @@ analyze_io(struct zink_shader *zs, nir_shader *shader)
             ret = true;
             break;
          }
+         case nir_intrinsic_ssbo_atomic_fadd:
          case nir_intrinsic_ssbo_atomic_add:
          case nir_intrinsic_ssbo_atomic_imin:
          case nir_intrinsic_ssbo_atomic_umin:
@@ -2953,6 +2954,56 @@ match_tex_dests(nir_shader *shader)
    return nir_shader_instructions_pass(shader, match_tex_dests_instr, nir_metadata_dominance, NULL);
 }
 
+static bool
+split_bitfields_instr(nir_builder *b, nir_instr *in, void *data)
+{
+   if (in->type != nir_instr_type_alu)
+      return false;
+   nir_alu_instr *alu = nir_instr_as_alu(in);
+   switch (alu->op) {
+   case nir_op_ubitfield_extract:
+   case nir_op_ibitfield_extract:
+   case nir_op_bitfield_insert:
+      break;
+   default:
+      return false;
+   }
+   unsigned num_components = nir_dest_num_components(alu->dest.dest);
+   if (num_components == 1)
+      return false;
+   b->cursor = nir_before_instr(in);
+   nir_ssa_def *dests[NIR_MAX_VEC_COMPONENTS];
+   for (unsigned i = 0; i < num_components; i++) {
+      if (alu->op == nir_op_bitfield_insert)
+         dests[i] = nir_bitfield_insert(b,
+                                        nir_channel(b, alu->src[0].src.ssa, alu->src[0].swizzle[i]),
+                                        nir_channel(b, alu->src[1].src.ssa, alu->src[1].swizzle[i]),
+                                        nir_channel(b, alu->src[2].src.ssa, alu->src[2].swizzle[i]),
+                                        nir_channel(b, alu->src[3].src.ssa, alu->src[3].swizzle[i]));
+      else if (alu->op == nir_op_ubitfield_extract)
+         dests[i] = nir_ubitfield_extract(b,
+                                          nir_channel(b, alu->src[0].src.ssa, alu->src[0].swizzle[i]),
+                                          nir_channel(b, alu->src[1].src.ssa, alu->src[1].swizzle[i]),
+                                          nir_channel(b, alu->src[2].src.ssa, alu->src[2].swizzle[i]));
+      else
+         dests[i] = nir_ibitfield_extract(b,
+                                          nir_channel(b, alu->src[0].src.ssa, alu->src[0].swizzle[i]),
+                                          nir_channel(b, alu->src[1].src.ssa, alu->src[1].swizzle[i]),
+                                          nir_channel(b, alu->src[2].src.ssa, alu->src[2].swizzle[i]));
+   }
+   nir_ssa_def *dest = nir_vec(b, dests, num_components);
+   nir_ssa_def_rewrite_uses_after(&alu->dest.dest.ssa, dest, in);
+   nir_instr_remove(in);
+   return true;
+}
+
+
+static bool
+split_bitfields(nir_shader *shader)
+{
+   return nir_shader_instructions_pass(shader, split_bitfields_instr, nir_metadata_dominance, NULL);
+}
+
 struct zink_shader *
 zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
                    const struct pipe_stream_output_info *so_info)
@@ -2991,6 +3042,7 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    NIR_PASS_V(nir, nir_lower_regs_to_ssa);
    NIR_PASS_V(nir, lower_baseinstance);
    NIR_PASS_V(nir, lower_sparse);
+   NIR_PASS_V(nir, split_bitfields);
 
    if (screen->need_2D_zs)
       NIR_PASS_V(nir, lower_1d_shadow, screen);
@@ -3125,6 +3177,8 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    NIR_PASS_V(nir, match_tex_dests);
 
    ret->nir = nir;
+   nir_foreach_shader_out_variable(var, nir)
+      var->data.explicit_xfb_buffer = 0;
    if (so_info && so_info->num_outputs)
       update_so_info(ret, so_info, nir->info.outputs_written, have_psiz);
    else if (have_psiz) {
diff --git a/src/gallium/drivers/zink/zink_context.c b/src/gallium/drivers/zink/zink_context.c
index 1bb8a4dd6b7..59563c6f86f 100644
--- a/src/gallium/drivers/zink/zink_context.c
+++ b/src/gallium/drivers/zink/zink_context.c
@@ -800,7 +800,7 @@ create_bvci(struct zink_context *ctx, struct zink_resource *res, enum pipe_forma
       if (bvci.offset + bvci.range >= res->base.b.width0)
          bvci.range = VK_WHOLE_SIZE;
    }
-   uint32_t clamp = blocksize * screen->info.props.limits.maxTexelBufferElements;
+   uint64_t clamp = blocksize * screen->info.props.limits.maxTexelBufferElements;
    if (bvci.range == VK_WHOLE_SIZE && res->base.b.width0 > clamp)
       bvci.range = clamp;
    bvci.flags = 0;
@@ -2775,8 +2775,10 @@ unbind_fb_surface(struct zink_context *ctx, struct pipe_surface *surf, unsigned
    res->fb_binds--;
    if (!res->fb_binds) {
       check_resource_for_batch_ref(ctx, res);
-      if (res->sampler_bind_count[0])
+      if (res->sampler_bind_count[0]) {
          update_res_sampler_layouts(ctx, res);
+         _mesa_set_add(ctx->need_barriers[0], res);
+      }
    }
 }
 
@@ -4192,11 +4194,11 @@ zink_resource_copy_region(struct pipe_context *pctx,
       region.extent.height = src_box->height;
 
       struct zink_batch *batch = &ctx->batch;
+      zink_resource_setup_transfer_layouts(ctx, src, dst);
       VkCommandBuffer cmdbuf = zink_get_cmdbuf(ctx, src, dst);
       zink_batch_reference_resource_rw(batch, src, false);
       zink_batch_reference_resource_rw(batch, dst, true);
 
-      zink_resource_setup_transfer_layouts(ctx, src, dst);
       VKCTX(CmdCopyImage)(cmdbuf, src->obj->image, src->layout,
                      dst->obj->image, dst->layout,
                      1, &region);
diff --git a/src/gallium/drivers/zink/zink_screen.c b/src/gallium/drivers/zink/zink_screen.c
index bba0aec8f8e..54d44714849 100644
--- a/src/gallium/drivers/zink/zink_screen.c
+++ b/src/gallium/drivers/zink/zink_screen.c
@@ -761,8 +761,8 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_MAX_SHADER_BUFFER_SIZE_UINT:
       /* 1<<27 is required by VK spec */
       assert(screen->info.props.limits.maxStorageBufferRange >= 1 << 27);
-      /* but Gallium can't handle values that are too big, so clamp to VK spec minimum */
-      return MIN2(get_smallest_buffer_heap(screen), 1 << 27);
+      /* clamp to VK spec minimum */
+      return MIN2(get_smallest_buffer_heap(screen), screen->info.props.limits.maxStorageBufferRange);
 
    case PIPE_CAP_FS_COORD_ORIGIN_UPPER_LEFT:
    case PIPE_CAP_FS_COORD_PIXEL_CENTER_HALF_INTEGER:
@@ -1266,6 +1266,8 @@ zink_destroy_screen(struct pipe_screen *pscreen)
       VKSCR(DestroyDebugUtilsMessengerEXT)(screen->instance, screen->debugUtilsCallbackHandle, NULL);
    }
 
+   util_vertex_state_cache_deinit(&screen->vertex_state_cache);
+
    u_transfer_helper_destroy(pscreen->transfer_helper);
 #ifdef ENABLE_SHADER_CACHE
    if (screen->disk_cache) {
diff --git a/src/gallium/frontends/lavapipe/lvp_execute.c b/src/gallium/frontends/lavapipe/lvp_execute.c
index 1602e577a8c..8885f40b8e1 100644
--- a/src/gallium/frontends/lavapipe/lvp_execute.c
+++ b/src/gallium/frontends/lavapipe/lvp_execute.c
@@ -378,7 +378,7 @@ static void emit_compute_state(struct rendering_state *state)
    if (state->sb_dirty[PIPE_SHADER_COMPUTE]) {
       state->pctx->set_shader_buffers(state->pctx, PIPE_SHADER_COMPUTE,
                                       0, state->num_shader_buffers[PIPE_SHADER_COMPUTE],
-                                      state->sb[PIPE_SHADER_COMPUTE], 0);
+                                      state->sb[PIPE_SHADER_COMPUTE], state->access[MESA_SHADER_COMPUTE].buffers_written);
       state->sb_dirty[PIPE_SHADER_COMPUTE] = false;
    }
 
@@ -1528,7 +1528,7 @@ static struct pipe_surface *create_img_surface_bo(struct rendering_state *state,
    template.width = width;
    template.height = height;
    template.u.tex.first_layer = range->baseArrayLayer + base_layer;
-   template.u.tex.last_layer = range->baseArrayLayer + layer_count;
+   template.u.tex.last_layer = range->baseArrayLayer + base_layer + layer_count - 1;
    template.u.tex.level = range->baseMipLevel + level;
 
    if (template.format == PIPE_FORMAT_NONE)
@@ -1552,12 +1552,20 @@ static struct pipe_surface *create_img_surface(struct rendering_state *state,
 }
 
 static void add_img_view_surface(struct rendering_state *state,
-                                 struct lvp_image_view *imgv, int width, int height)
+                                 struct lvp_image_view *imgv, int width, int height,
+                                 int layer_count)
 {
+   if (imgv->surface) {
+      if (imgv->surface->width != width ||
+          imgv->surface->height != height ||
+          (imgv->surface->u.tex.last_layer - imgv->surface->u.tex.first_layer) != (layer_count - 1))
+         pipe_surface_reference(&imgv->surface, NULL);
+   }
+
    if (!imgv->surface) {
       imgv->surface = create_img_surface(state, imgv, imgv->vk.format,
                                          width, height,
-                                         0, imgv->vk.layer_count - 1);
+                                         0, layer_count);
    }
 }
 
@@ -1589,7 +1597,7 @@ static void clear_attachment_layers(struct rendering_state *state,
                                                         state->framebuffer.width,
                                                         state->framebuffer.height,
                                                         base_layer,
-                                                        base_layer + layer_count - 1);
+                                                        layer_count);
 
    if (ds_clear_flags) {
       state->pctx->clear_depth_stencil(state->pctx,
@@ -1968,7 +1976,7 @@ static void handle_begin_rendering(struct vk_cmd_queue_entry *cmd,
                               info->renderArea.extent.width;
    state->framebuffer.height = info->renderArea.offset.y +
                                info->renderArea.extent.height;
-   state->framebuffer.layers = info->layerCount;
+   state->framebuffer.layers = info->viewMask ? util_last_bit(info->viewMask) : info->layerCount;
    state->framebuffer.nr_cbufs = info->colorAttachmentCount;
 
    state->color_att_count = info->colorAttachmentCount;
@@ -1978,7 +1986,8 @@ static void handle_begin_rendering(struct vk_cmd_queue_entry *cmd,
       if (state->color_att[i].imgv) {
          struct lvp_image_view *imgv = state->color_att[i].imgv;
          add_img_view_surface(state, imgv,
-                              state->framebuffer.width, state->framebuffer.height);
+                              state->framebuffer.width, state->framebuffer.height,
+                              state->framebuffer.layers);
          if (state->forced_sample_count && imgv->image->vk.samples == 1)
             state->color_att[i].imgv = create_multisample_surface(state, imgv, state->forced_sample_count,
                                                                   att_needs_replicate(state, imgv, state->color_att[i].load_op));
@@ -1998,7 +2007,8 @@ static void handle_begin_rendering(struct vk_cmd_queue_entry *cmd,
                                                state->stencil_att.imgv;
       struct lvp_image_view *imgv = state->ds_imgv;
       add_img_view_surface(state, imgv,
-                           state->framebuffer.width, state->framebuffer.height);
+                           state->framebuffer.width, state->framebuffer.height,
+                           state->framebuffer.layers);
       if (state->forced_sample_count && imgv->image->vk.samples == 1) {
          VkAttachmentLoadOp load_op;
          if (state->depth_att.load_op == VK_ATTACHMENT_LOAD_OP_CLEAR ||
@@ -3095,15 +3105,20 @@ static void handle_clear_ds_image(struct vk_cmd_queue_entry *cmd,
       uint32_t level_count = vk_image_subresource_level_count(&image->vk, range);
       for (unsigned j = 0; j < level_count; j++) {
          struct pipe_surface *surf;
-         unsigned width, height;
-
+         unsigned width, height, depth;
          width = u_minify(image->bo->width0, range->baseMipLevel + j);
          height = u_minify(image->bo->height0, range->baseMipLevel + j);
 
+         if (image->bo->target == PIPE_TEXTURE_3D)
+            depth = u_minify(image->bo->depth0, range->baseMipLevel + j);
+         else {
+            depth = vk_image_subresource_layer_count(&image->vk, range);
+         }
+
          surf = create_img_surface_bo(state, range,
                                       image->bo, image->bo->format,
                                       width, height,
-                                      0, vk_image_subresource_layer_count(&image->vk, range) - 1, j);
+                                      0, depth, j);
 
          state->pctx->clear_depth_stencil(state->pctx,
                                           surf,
diff --git a/src/gallium/frontends/lavapipe/lvp_pipeline.c b/src/gallium/frontends/lavapipe/lvp_pipeline.c
index cf523f2f81a..2a0c3bc2a5d 100644
--- a/src/gallium/frontends/lavapipe/lvp_pipeline.c
+++ b/src/gallium/frontends/lavapipe/lvp_pipeline.c
@@ -790,6 +790,7 @@ lvp_graphics_pipeline_init(struct lvp_pipeline *pipeline,
             pipeline->line_rectangular = p->line_rectangular;
             pipeline->last_vertex = p->last_vertex;
             memcpy(&pipeline->stream_output, &p->stream_output, sizeof(p->stream_output));
+            memcpy(&pipeline->access, &p->access, sizeof(p->access));
          }
          if (p->stages & VK_GRAPHICS_PIPELINE_LIBRARY_FRAGMENT_SHADER_BIT_EXT)
             pipeline->force_min_sample = p->force_min_sample;
diff --git a/src/gallium/frontends/va/context.c b/src/gallium/frontends/va/context.c
index b205d173ba0..c70e826148e 100644
--- a/src/gallium/frontends/va/context.c
+++ b/src/gallium/frontends/va/context.c
@@ -35,7 +35,9 @@
 #include "vl/vl_winsys.h"
 
 #include "va_private.h"
+#ifdef HAVE_DRISW_KMS
 #include "loader/loader.h"
+#endif
 
 #include <va/va_drmcommon.h>
 
@@ -142,12 +144,14 @@ VA_DRIVER_INIT_FUNC(VADriverContextP ctx)
          FREE(drv);
          return VA_STATUS_ERROR_INVALID_PARAMETER;
       }
+#ifdef HAVE_DRISW_KMS
       char* drm_driver_name = loader_get_driver_for_fd(drm_info->fd);
       if(drm_driver_name) {
          if (strcmp(drm_driver_name, "vgem") == 0)
             drv->vscreen = vl_vgem_drm_screen_create(drm_info->fd);
          FREE(drm_driver_name);
       }
+#endif
       if(!drv->vscreen)
          drv->vscreen = vl_drm_screen_create(drm_info->fd);
       break;
diff --git a/src/gallium/winsys/virgl/vtest/virgl_vtest_socket.c b/src/gallium/winsys/virgl/vtest/virgl_vtest_socket.c
index f24116e32da..62ef471405a 100644
--- a/src/gallium/winsys/virgl/vtest/virgl_vtest_socket.c
+++ b/src/gallium/winsys/virgl/vtest/virgl_vtest_socket.c
@@ -253,8 +253,13 @@ int virgl_vtest_send_get_caps(struct virgl_vtest_winsys *vws,
 
        ret = virgl_block_read(vws->sock_fd, &caps->caps, resp_size);
 
-       if (dummy_size)
-	   ret = virgl_block_read(vws->sock_fd, &dummy, dummy_size);
+       while (dummy_size) {
+           ret = virgl_block_read(vws->sock_fd, &dummy,
+                    dummy_size < sizeof(dummy) ? dummy_size : sizeof(dummy));
+           if (ret <= 0)
+               break;
+           dummy_size -= ret;
+       }
 
        /* now read back the pointless caps v1 we requested */
        ret = virgl_block_read(vws->sock_fd, resp_buf, sizeof(resp_buf));
diff --git a/src/intel/compiler/brw_fs.cpp b/src/intel/compiler/brw_fs.cpp
index a7850b5db7d..0f282dcd345 100644
--- a/src/intel/compiler/brw_fs.cpp
+++ b/src/intel/compiler/brw_fs.cpp
@@ -6153,7 +6153,7 @@ fs_visitor::optimize()
       OPT(split_virtual_grfs);
 
       /* Lower 64 bit MOVs generated by payload lowering. */
-      if (!devinfo->has_64bit_float && !devinfo->has_64bit_int)
+      if (!devinfo->has_64bit_float || !devinfo->has_64bit_int)
          OPT(opt_algebraic);
 
       OPT(register_coalesce);
diff --git a/src/intel/compiler/brw_lower_logical_sends.cpp b/src/intel/compiler/brw_lower_logical_sends.cpp
index eea9492ef62..255ce759481 100644
--- a/src/intel/compiler/brw_lower_logical_sends.cpp
+++ b/src/intel/compiler/brw_lower_logical_sends.cpp
@@ -1910,7 +1910,7 @@ emit_a64_oword_block_header(const fs_builder &bld, const fs_reg &addr)
       /* We can't do stride 1 with the UNIFORM file, it requires stride 0 */
       expanded_addr = ubld.vgrf(BRW_REGISTER_TYPE_UQ);
       expanded_addr.stride = 0;
-      ubld.MOV(expanded_addr, addr);
+      ubld.MOV(expanded_addr, retype(addr, BRW_REGISTER_TYPE_UQ));
    }
 
    fs_reg header = ubld.vgrf(BRW_REGISTER_TYPE_UD);
diff --git a/src/intel/compiler/brw_mesh.cpp b/src/intel/compiler/brw_mesh.cpp
index 3c2c63fe51b..61ac55ab362 100644
--- a/src/intel/compiler/brw_mesh.cpp
+++ b/src/intel/compiler/brw_mesh.cpp
@@ -530,8 +530,6 @@ brw_nir_lower_mue_outputs(nir_shader *nir, const struct brw_mue_map *map)
 
    NIR_PASS(_, nir, nir_lower_io, nir_var_shader_out,
             type_size_scalar_dwords, nir_lower_io_lower_64bit_to_32);
-
-   NIR_PASS(_, nir, brw_nir_lower_shading_rate_output);
 }
 
 static void
diff --git a/src/intel/compiler/brw_nir_lower_ray_queries.c b/src/intel/compiler/brw_nir_lower_ray_queries.c
index 6aa61a35c5b..7ce34dfdcd3 100644
--- a/src/intel/compiler/brw_nir_lower_ray_queries.c
+++ b/src/intel/compiler/brw_nir_lower_ray_queries.c
@@ -36,6 +36,8 @@ struct lowering_state {
 
    struct brw_nir_rt_globals_defs globals;
    nir_ssa_def *rq_globals;
+
+   uint32_t state_scratch_base_offset;
 };
 
 struct brw_ray_query {
@@ -43,6 +45,8 @@ struct brw_ray_query {
    uint32_t id;
 };
 
+#define SIZEOF_QUERY_STATE (sizeof(uint32_t))
+
 static bool
 need_spill_fill(struct lowering_state *state)
 {
@@ -91,7 +95,7 @@ static nir_ssa_def *
 get_ray_query_shadow_addr(nir_builder *b,
                           nir_deref_instr *deref,
                           struct lowering_state *state,
-                          nir_ssa_def **out_state_addr)
+                          nir_ssa_def **out_state_offset)
 {
    nir_deref_path path;
    nir_deref_path_init(&path, deref, NULL);
@@ -111,14 +115,8 @@ get_ray_query_shadow_addr(nir_builder *b,
                    brw_rt_ray_queries_shadow_stack_size(state->devinfo) * rq->id);
 
    bool spill_fill = need_spill_fill(state);
-   *out_state_addr =
-      spill_fill ?
-      nir_iadd_imm(b,
-                   state->globals.resume_sbt_addr,
-                   brw_rt_ray_queries_shadow_stack_size(state->devinfo) *
-                   b->shader->info.ray_queries +
-                   4 * rq->id) :
-      state->globals.resume_sbt_addr;
+   *out_state_offset = nir_imm_int(b, state->state_scratch_base_offset +
+                                      SIZEOF_QUERY_STATE * rq->id);
 
    if (!spill_fill)
       return NULL;
@@ -130,11 +128,11 @@ get_ray_query_shadow_addr(nir_builder *b,
          nir_ssa_def *index = nir_ssa_for_src(b, (*p)->arr.index, 1);
 
          /**/
-         uint32_t local_state_offset = 4 * MAX2(1, glsl_get_aoa_size((*p)->type));
-         *out_state_addr =
-            nir_iadd(b, *out_state_addr,
-                        nir_i2i64(b,
-                                  nir_imul_imm(b, index, local_state_offset)));
+         uint32_t local_state_offset = SIZEOF_QUERY_STATE *
+                                       MAX2(1, glsl_get_aoa_size((*p)->type));
+         *out_state_offset =
+            nir_iadd(b, *out_state_offset,
+                        nir_imul_imm(b, index, local_state_offset));
 
          /**/
          uint64_t size = MAX2(1, glsl_get_aoa_size((*p)->type)) *
@@ -168,13 +166,13 @@ get_ray_query_shadow_addr(nir_builder *b,
 
 static void
 update_trace_ctrl_level(nir_builder *b,
-                        nir_ssa_def *state_addr,
+                        nir_ssa_def *state_scratch_offset,
                         nir_ssa_def **out_old_ctrl,
                         nir_ssa_def **out_old_level,
                         nir_ssa_def *new_ctrl,
                         nir_ssa_def *new_level)
 {
-   nir_ssa_def *old_value = brw_nir_rt_load(b, state_addr, 4, 1, 32);
+   nir_ssa_def *old_value = nir_load_scratch(b, 1, 32, state_scratch_offset, 4);
    nir_ssa_def *old_ctrl = nir_ishr_imm(b, old_value, 2);
    nir_ssa_def *old_level = nir_iand_imm(b, old_value, 0x3);
 
@@ -190,7 +188,7 @@ update_trace_ctrl_level(nir_builder *b,
          new_level = old_level;
 
       nir_ssa_def *new_value = nir_ior(b, nir_ishl_imm(b, new_ctrl, 2), new_level);
-      brw_nir_rt_store(b, state_addr, 4, new_value, 0x1);
+      nir_store_scratch(b, new_value, state_scratch_offset, 4, 0x1);
    }
 }
 
@@ -200,20 +198,8 @@ fill_query(nir_builder *b,
            nir_ssa_def *shadow_stack_addr,
            nir_ssa_def *ctrl)
 {
-   brw_nir_memcpy_global(b,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, hw_stack_addr, false), 16,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, shadow_stack_addr, false), 16,
-                         BRW_RT_SIZEOF_HIT_INFO);
-   brw_nir_memcpy_global(b,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, hw_stack_addr, true), 16,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, shadow_stack_addr, true), 16,
-                         BRW_RT_SIZEOF_HIT_INFO);
-   brw_nir_memcpy_global(b,
-                         brw_nir_rt_mem_ray_addr(b, hw_stack_addr,
-                                                 BRW_RT_BVH_LEVEL_WORLD), 16,
-                         brw_nir_rt_mem_ray_addr(b, shadow_stack_addr,
-                                                 BRW_RT_BVH_LEVEL_WORLD), 16,
-                         BRW_RT_SIZEOF_RAY);
+   brw_nir_memcpy_global(b, hw_stack_addr, 64, shadow_stack_addr, 64,
+                         BRW_RT_SIZEOF_RAY_QUERY);
 }
 
 static void
@@ -221,24 +207,8 @@ spill_query(nir_builder *b,
             nir_ssa_def *hw_stack_addr,
             nir_ssa_def *shadow_stack_addr)
 {
-   struct brw_nir_rt_mem_hit_defs committed_hit = {};
-   brw_nir_rt_load_mem_hit_from_addr(b, &committed_hit, hw_stack_addr, true);
-
-   /* Always copy the potential hit back */
-   brw_nir_memcpy_global(b,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, shadow_stack_addr, false), 16,
-                         brw_nir_rt_mem_hit_addr_from_addr(b, hw_stack_addr, false), 16,
-                         BRW_RT_SIZEOF_HIT_INFO);
-
-   /* Also copy the committed hit back if it is valid */
-   nir_push_if(b, committed_hit.valid);
-   {
-      brw_nir_memcpy_global(b,
-                            brw_nir_rt_mem_hit_addr_from_addr(b, shadow_stack_addr, true), 16,
-                            brw_nir_rt_mem_hit_addr_from_addr(b, hw_stack_addr, true), 16,
-                            BRW_RT_SIZEOF_HIT_INFO);
-   }
-   nir_pop_if(b, NULL);
+   brw_nir_memcpy_global(b, shadow_stack_addr, 64, hw_stack_addr, 64,
+                         BRW_RT_SIZEOF_RAY_QUERY);
 }
 
 
@@ -339,7 +309,7 @@ lower_ray_query_intrinsic(nir_builder *b,
          update_trace_ctrl_level(b, ctrl_level_addr,
                                  NULL, NULL,
                                  nir_imm_int(b, GEN_RT_TRACE_RAY_CONTINUE),
-                                 nir_imm_int(b, BRW_RT_BVH_LEVEL_OBJECT));
+                                 hit_in.bvh_level);
 
          not_done_then = nir_inot(b, hit_in.done);
       }
@@ -568,15 +538,20 @@ brw_nir_lower_ray_queries(nir_shader *shader,
          maybe_create_brw_var(instr, &state);
    }
 
-   bool progress = _mesa_hash_table_num_entries(state.queries) > 0;
+   bool progress = state.n_queries > 0;
 
    if (progress) {
+      state.state_scratch_base_offset = shader->scratch_size;
+      shader->scratch_size += SIZEOF_QUERY_STATE * state.n_queries;
+
       lower_ray_query_impl(impl, &state);
 
       nir_remove_dead_derefs(shader);
       nir_remove_dead_variables(shader,
                                 nir_var_shader_temp | nir_var_function_temp,
                                 NULL);
+
+      nir_metadata_preserve(impl, nir_metadata_none);
    }
 
    ralloc_free(state.queries);
diff --git a/src/intel/compiler/brw_nir_lower_shader_calls.c b/src/intel/compiler/brw_nir_lower_shader_calls.c
index 9a7f330e70c..19ef08e49ab 100644
--- a/src/intel/compiler/brw_nir_lower_shader_calls.c
+++ b/src/intel/compiler/brw_nir_lower_shader_calls.c
@@ -25,6 +25,28 @@
 #include "brw_nir_rt_builder.h"
 #include "nir_phi_builder.h"
 
+UNUSED static bool
+no_load_scratch_base_ptr_intrinsic(nir_shader *shader)
+{
+   nir_foreach_function(func, shader) {
+      if (!func->impl)
+         continue;
+
+      nir_foreach_block(block, func->impl) {
+         nir_foreach_instr(instr, block) {
+            if (instr->type != nir_instr_type_intrinsic)
+               continue;
+
+            nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+            if (intrin->intrinsic == nir_intrinsic_load_scratch_base_ptr)
+               return false;
+         }
+      }
+   }
+
+   return true;
+}
+
 /** Insert the appropriate return instruction at the end of the shader */
 void
 brw_nir_lower_shader_returns(nir_shader *shader)
@@ -46,9 +68,9 @@ brw_nir_lower_shader_returns(nir_shader *shader)
     * This isn't needed for ray-gen shaders because they end the thread and
     * never return to the calling trampoline shader.
     */
-   assert(shader->scratch_size == 0);
+   assert(no_load_scratch_base_ptr_intrinsic(shader));
    if (shader->info.stage != MESA_SHADER_RAYGEN)
-      shader->scratch_size = BRW_BTD_STACK_CALLEE_DATA_SIZE;
+      shader->scratch_size += BRW_BTD_STACK_CALLEE_DATA_SIZE;
 
    nir_builder b;
    nir_builder_init(&b, impl);
diff --git a/src/intel/compiler/brw_nir_rt.c b/src/intel/compiler/brw_nir_rt.c
index ad529cdfcce..b124928b293 100644
--- a/src/intel/compiler/brw_nir_rt.c
+++ b/src/intel/compiler/brw_nir_rt.c
@@ -489,6 +489,7 @@ brw_nir_create_raygen_trampoline(const struct brw_compiler *compiler,
 
    NIR_PASS_V(nir, brw_nir_lower_rt_intrinsics, devinfo);
 
+   nir_builder_init(&b, nir_shader_get_entrypoint(b.shader));
    /* brw_nir_lower_rt_intrinsics will leave us with a btd_global_arg_addr
     * intrinsic which doesn't exist in compute shaders.  We also created one
     * above when we generated the BTD spawn intrinsic.  Now we go through and
diff --git a/src/intel/compiler/brw_nir_rt_builder.h b/src/intel/compiler/brw_nir_rt_builder.h
index c7eb23f3244..16673901582 100644
--- a/src/intel/compiler/brw_nir_rt_builder.h
+++ b/src/intel/compiler/brw_nir_rt_builder.h
@@ -90,8 +90,6 @@ brw_load_eu_thread_simd(nir_builder *b)
 static inline nir_ssa_def *
 brw_nir_rt_async_stack_id(nir_builder *b)
 {
-   assert(gl_shader_stage_is_callable(b->shader->info.stage) ||
-          b->shader->info.stage == MESA_SHADER_RAYGEN);
    return nir_iadd(b, nir_umul_32x16(b, nir_load_ray_num_dss_rt_stacks_intel(b),
                                         brw_load_btd_dss_id(b)),
                       nir_load_btd_stack_id_intel(b));
@@ -147,7 +145,6 @@ brw_nir_btd_retire(nir_builder *b)
 static inline void
 brw_nir_btd_return(struct nir_builder *b)
 {
-   assert(b->shader->scratch_size == BRW_BTD_STACK_CALLEE_DATA_SIZE);
    nir_ssa_def *resume_addr =
       brw_nir_rt_load_scratch(b, BRW_BTD_STACK_RESUME_BSR_ADDR_OFFSET,
                               8 /* align */, 1, 64);
@@ -374,6 +371,25 @@ brw_nir_rt_unpack_leaf_ptr(nir_builder *b, nir_ssa_def *vec2)
    return nir_pack_64_2x32_split(b, ptr_lo, ptr_hi);
 }
 
+/**
+ * MemHit memory layout (BSpec 47547) :
+ *
+ *      name            bits    description
+ *    - t               32      hit distance of current hit (or initial traversal distance)
+ *    - u               32      barycentric hit coordinates
+ *    - v               32      barycentric hit coordinates
+ *    - primIndexDelta  16      prim index delta for compressed meshlets and quads
+ *    - valid            1      set if there is a hit
+ *    - leafType         3      type of node primLeafPtr is pointing to
+ *    - primLeafIndex    4      index of the hit primitive inside the leaf
+ *    - bvhLevel         3      the instancing level at which the hit occured
+ *    - frontFace        1      whether we hit the front-facing side of a triangle (also used to pass opaque flag when calling intersection shaders)
+ *    - pad0             4      unused bits
+ *    - primLeafPtr     42      pointer to BVH leaf node (multiple of 64 bytes)
+ *    - hitGroupRecPtr0 22      LSB of hit group record of the hit triangle (multiple of 16 bytes)
+ *    - instLeafPtr     42      pointer to BVH instance leaf node (in multiple of 64 bytes)
+ *    - hitGroupRecPtr1 22      MSB of hit group record of the hit triangle (multiple of 32 bytes)
+ */
 struct brw_nir_rt_mem_hit_defs {
    nir_ssa_def *t;
    nir_ssa_def *tri_bary; /**< Only valid for triangle geometry */
@@ -574,41 +590,61 @@ brw_nir_rt_commit_hit(nir_builder *b)
 static inline void
 brw_nir_rt_generate_hit_addr(nir_builder *b, nir_ssa_def *stack_addr, nir_ssa_def *t_val)
 {
-   nir_ssa_def *dst_addr =
+   nir_ssa_def *committed_addr =
       brw_nir_rt_mem_hit_addr_from_addr(b, stack_addr, true /* committed */);
-   nir_ssa_def *src_addr =
+   nir_ssa_def *potential_addr =
       brw_nir_rt_mem_hit_addr_from_addr(b, stack_addr, false /* committed */);
 
-   /* Load 2 vec4 */
-   nir_ssa_def *potential_data[2] = {
-      brw_nir_rt_load(b, src_addr, 16, 4, 32),
-      brw_nir_rt_load(b, nir_iadd_imm(b, src_addr, 16), 16, 4, 32),
-   };
-
-   /* Update the potential hit distance */
-   brw_nir_rt_store(b, src_addr, 4, t_val, 0x1);
-   /* Also mark the potential hit as valid */
-   brw_nir_rt_store(b, nir_iadd_imm(b, src_addr, 12), 4,
-                    nir_ior_imm(b, nir_channel(b, potential_data[0], 3),
-                                   (0x1 << 16) /* valid */), 0x1);
+   /* Set:
+    *
+    *   potential.t     = t_val;
+    *   potential.valid = true;
+    */
+   nir_ssa_def *potential_hit_dwords_0_3 =
+      brw_nir_rt_load(b, potential_addr, 16, 4, 32);
+   potential_hit_dwords_0_3 =
+      nir_vec4(b,
+               t_val,
+               nir_channel(b, potential_hit_dwords_0_3, 1),
+               nir_channel(b, potential_hit_dwords_0_3, 2),
+               nir_ior_imm(b, nir_channel(b, potential_hit_dwords_0_3, 3),
+                           (0x1 << 16) /* valid */));
+   brw_nir_rt_store(b, potential_addr, 16, potential_hit_dwords_0_3, 0xf /* write_mask */);
 
-   /* Now write the committed hit. */
-   nir_ssa_def *committed_data[2] = {
+   /* Set:
+    *
+    *   committed.t               = t_val;
+    *   committed.u               = 0.0f;
+    *   committed.v               = 0.0f;
+    *   committed.valid           = true;
+    *   committed.leaf_type       = potential.leaf_type;
+    *   committed.bvh_level       = BRW_RT_BVH_LEVEL_OBJECT;
+    *   committed.front_face      = false;
+    *   committed.prim_leaf_index = 0;
+    *   committed.done            = false;
+    */
+   nir_ssa_def *committed_hit_dwords_0_3 =
+      brw_nir_rt_load(b, committed_addr, 16, 4, 32);
+   committed_hit_dwords_0_3 =
       nir_vec4(b,
                t_val,
-               nir_imm_float(b, 0.0f), /* barycentric */
-               nir_imm_float(b, 0.0f), /* barycentric */
+               nir_imm_float(b, 0.0f),
+               nir_imm_float(b, 0.0f),
                nir_ior_imm(b,
-                           /* Just keep leaf_type */
-                           nir_iand_imm(b, nir_channel(b, potential_data[0], 3), 0x0000e000),
-                           (0x1 << 16) /* valid */ |
-                           (BRW_RT_BVH_LEVEL_OBJECT << 5))),
-      potential_data[1],
-   };
+                           nir_ior_imm(b, nir_channel(b, potential_hit_dwords_0_3, 3), 0x000e0000),
+                           (0x1 << 16)                     /* valid */ |
+                           (BRW_RT_BVH_LEVEL_OBJECT << 24) /* leaf_type */));
+   brw_nir_rt_store(b, committed_addr, 16, committed_hit_dwords_0_3, 0xf /* write_mask */);
 
-   brw_nir_rt_store(b, dst_addr, 16, committed_data[0], 0xf /* write_mask */);
-   brw_nir_rt_store(b, nir_iadd_imm(b, dst_addr, 16), 16,
-                    committed_data[1], 0xf /* write_mask */);
+   /* Set:
+    *
+    *   committed.prim_leaf_ptr   = potential.prim_leaf_ptr;
+    *   committed.inst_leaf_ptr   = potential.inst_leaf_ptr;
+    */
+   brw_nir_memcpy_global(b,
+                         nir_iadd_imm(b, committed_addr, 16), 16,
+                         nir_iadd_imm(b, potential_addr, 16), 16,
+                         16);
 }
 
 struct brw_nir_rt_mem_ray_defs {
diff --git a/src/intel/vulkan/anv_descriptor_set.c b/src/intel/vulkan/anv_descriptor_set.c
index c8fe93a9fbd..b2ecf0d447f 100644
--- a/src/intel/vulkan/anv_descriptor_set.c
+++ b/src/intel/vulkan/anv_descriptor_set.c
@@ -1305,9 +1305,20 @@ VkResult anv_AllocateDescriptorSets(
       pDescriptorSets[i] = anv_descriptor_set_to_handle(set);
    }
 
-   if (result != VK_SUCCESS)
+   if (result != VK_SUCCESS) {
       anv_FreeDescriptorSets(_device, pAllocateInfo->descriptorPool,
                              i, pDescriptorSets);
+      /* The Vulkan 1.3.228 spec, section 14.2.3. Allocation of Descriptor Sets:
+       *
+       *   "If the creation of any of those descriptor sets fails, then the
+       *    implementation must destroy all successfully created descriptor
+       *    set objects from this command, set all entries of the
+       *    pDescriptorSets array to VK_NULL_HANDLE and return the error."
+       */
+      for (i = 0; i < pAllocateInfo->descriptorSetCount; i++)
+         pDescriptorSets[i] = VK_NULL_HANDLE;
+
+   }
 
    return result;
 }
diff --git a/src/intel/vulkan/anv_private.h b/src/intel/vulkan/anv_private.h
index 5d95e7762de..f668355b82c 100644
--- a/src/intel/vulkan/anv_private.h
+++ b/src/intel/vulkan/anv_private.h
@@ -2285,7 +2285,6 @@ enum anv_pipe_bits {
    ANV_PIPE_STATE_CACHE_INVALIDATE_BIT | \
    ANV_PIPE_CONSTANT_CACHE_INVALIDATE_BIT | \
    ANV_PIPE_VF_CACHE_INVALIDATE_BIT | \
-   ANV_PIPE_HDC_PIPELINE_FLUSH_BIT | \
    ANV_PIPE_TEXTURE_CACHE_INVALIDATE_BIT | \
    ANV_PIPE_INSTRUCTION_CACHE_INVALIDATE_BIT | \
    ANV_PIPE_AUX_TABLE_INVALIDATE_BIT)
diff --git a/src/intel/vulkan/genX_cmd_buffer.c b/src/intel/vulkan/genX_cmd_buffer.c
index 64c6f183ae2..240630d8412 100644
--- a/src/intel/vulkan/genX_cmd_buffer.c
+++ b/src/intel/vulkan/genX_cmd_buffer.c
@@ -286,6 +286,18 @@ genX(cmd_buffer_emit_state_base_address)(struct anv_cmd_buffer *cmd_buffer)
       pc.StateCacheInvalidationEnable = true;
 #if GFX_VERx10 == 125
       pc.InstructionCacheInvalidateEnable = true;
+#endif
+#if GFX_VER >= 9 && GFX_VER <= 11
+      /* From the SKL PRM, Vol. 2a, "PIPE_CONTROL",
+       *
+       *    "Workaround : “CS Stall” bit in PIPE_CONTROL command must be
+       *     always set for GPGPU workloads when “Texture Cache Invalidation
+       *     Enable” bit is set".
+       *
+       * Workaround stopped appearing in TGL PRMs.
+       */
+      pc.CommandStreamerStallEnable =
+         cmd_buffer->state.current_pipeline == GPGPU;
 #endif
       anv_debug_dump_pc(pc);
    }
@@ -2269,6 +2281,19 @@ genX(emit_apply_pipe_flushes)(struct anv_batch *batch,
          pipe.InstructionCacheInvalidateEnable =
             bits & ANV_PIPE_INSTRUCTION_CACHE_INVALIDATE_BIT;
 
+#if GFX_VER >= 9 && GFX_VER <= 11
+         /* From the SKL PRM, Vol. 2a, "PIPE_CONTROL",
+          *
+          *    "Workaround : “CS Stall” bit in PIPE_CONTROL command must be
+          *     always set for GPGPU workloads when “Texture Cache
+          *     Invalidation Enable” bit is set".
+          *
+          * Workaround stopped appearing in TGL PRMs.
+          */
+         if (current_pipeline == GPGPU && pipe.TextureCacheInvalidationEnable)
+            pipe.CommandStreamerStallEnable = true;
+#endif
+
          /* From the SKL PRM, Vol. 2a, "PIPE_CONTROL",
           *
           *    "When VF Cache Invalidate is set “Post Sync Operation” must be
diff --git a/src/mesa/main/debug_output.c b/src/mesa/main/debug_output.c
index fbf414ffbf1..bee71950dea 100644
--- a/src/mesa/main/debug_output.c
+++ b/src/mesa/main/debug_output.c
@@ -1089,7 +1089,7 @@ _mesa_DebugMessageInsert(GLenum source, GLenum type, GLuint id,
                  gl_enum_to_debug_severity(severity),
                  length, buf);
 
-   if (type == GL_DEBUG_TYPE_MARKER && ctx->pipe->emit_string_marker) {
+   if (type == GL_DEBUG_TYPE_MARKER && ctx->has_string_marker) {
       ctx->pipe->emit_string_marker(ctx->pipe, buf, length);
    }
 }
diff --git a/src/mesa/main/glthread_bufferobj.c b/src/mesa/main/glthread_bufferobj.c
index 5745df4e0e4..23680e1e81a 100644
--- a/src/mesa/main/glthread_bufferobj.c
+++ b/src/mesa/main/glthread_bufferobj.c
@@ -208,7 +208,7 @@ _mesa_glthread_DeleteBuffers(struct gl_context *ctx, GLsizei n,
 {
    struct glthread_state *glthread = &ctx->GLThread;
 
-   if (!buffers)
+   if (!buffers || n < 0)
       return;
 
    for (unsigned i = 0; i < n; i++) {
diff --git a/src/mesa/main/mtypes.h b/src/mesa/main/mtypes.h
index eea350fbd72..18bc475a06c 100644
--- a/src/mesa/main/mtypes.h
+++ b/src/mesa/main/mtypes.h
@@ -3625,6 +3625,7 @@ struct gl_context
    struct st_config_options *st_opts;
    struct cso_context *cso_context;
    bool has_invalidate_buffer;
+   bool has_string_marker;
    /* On old libGL's for linux we need to invalidate the drawables
     * on glViewpport calls, this is set via a option.
     */
diff --git a/src/mesa/main/texturebindless.c b/src/mesa/main/texturebindless.c
index 62635fd94c8..c504ffa1811 100644
--- a/src/mesa/main/texturebindless.c
+++ b/src/mesa/main/texturebindless.c
@@ -240,6 +240,7 @@ new_texture_handle(struct gl_context *ctx, struct gl_texture_object *texObj,
                                                     true, false);
    } else {
       view = st_get_buffer_sampler_view_from_stobj(st, texObj, false);
+      sampler.normalized_coords = 1;
    }
 
    return pipe->create_texture_handle(pipe, view, &sampler);
diff --git a/src/mesa/state_tracker/st_cb_bitmap.c b/src/mesa/state_tracker/st_cb_bitmap.c
index e2f944765ed..5f74464a772 100644
--- a/src/mesa/state_tracker/st_cb_bitmap.c
+++ b/src/mesa/state_tracker/st_cb_bitmap.c
@@ -561,7 +561,8 @@ init_bitmap_state(struct st_context *st)
    st->bitmap.sampler.min_img_filter = PIPE_TEX_FILTER_NEAREST;
    st->bitmap.sampler.min_mip_filter = PIPE_TEX_MIPFILTER_NONE;
    st->bitmap.sampler.mag_img_filter = PIPE_TEX_FILTER_NEAREST;
-   st->bitmap.sampler.normalized_coords = st->internal_target == PIPE_TEXTURE_2D;
+   st->bitmap.sampler.normalized_coords = st->internal_target == PIPE_TEXTURE_2D ||
+                                          (st->internal_target == PIPE_TEXTURE_RECT && st->lower_rect_tex);
 
    st->bitmap.atlas_sampler = st->bitmap.sampler;
    st->bitmap.atlas_sampler.normalized_coords = 0;
diff --git a/src/mesa/state_tracker/st_cb_drawpixels.c b/src/mesa/state_tracker/st_cb_drawpixels.c
index 4af6c1e8628..9566a45a23d 100644
--- a/src/mesa/state_tracker/st_cb_drawpixels.c
+++ b/src/mesa/state_tracker/st_cb_drawpixels.c
@@ -747,7 +747,8 @@ draw_textured_quad(struct gl_context *ctx, GLint x, GLint y, GLfloat z,
    const unsigned fb_height = _mesa_geometric_height(ctx->DrawBuffer);
    GLfloat x0, y0, x1, y1;
    ASSERTED GLsizei maxSize;
-   boolean normalized = sv[0]->texture->target == PIPE_TEXTURE_2D;
+   boolean normalized = sv[0]->texture->target == PIPE_TEXTURE_2D ||
+                        (sv[0]->texture->target == PIPE_TEXTURE_RECT && st->lower_rect_tex);
    unsigned cso_state_mask;
 
    assert(sv[0]->texture->target == st->internal_target);
diff --git a/src/mesa/state_tracker/st_cb_readpixels.c b/src/mesa/state_tracker/st_cb_readpixels.c
index 6364d8f9444..5c96e1fb210 100644
--- a/src/mesa/state_tracker/st_cb_readpixels.c
+++ b/src/mesa/state_tracker/st_cb_readpixels.c
@@ -159,6 +159,7 @@ try_pbo_readpixels(struct st_context *st, struct gl_renderbuffer *rb,
       struct pipe_sampler_view templ;
       struct pipe_sampler_view *sampler_view;
       struct pipe_sampler_state sampler = {0};
+      sampler.normalized_coords = true;
       const struct pipe_sampler_state *samplers[1] = {&sampler};
 
       u_sampler_view_default_template(&templ, texture, src_format);
diff --git a/src/mesa/state_tracker/st_cb_texture.c b/src/mesa/state_tracker/st_cb_texture.c
index 485a056acfc..319b4bad068 100644
--- a/src/mesa/state_tracker/st_cb_texture.c
+++ b/src/mesa/state_tracker/st_cb_texture.c
@@ -1782,6 +1782,7 @@ try_pbo_download(struct st_context *st,
       struct pipe_sampler_view templ;
       struct pipe_sampler_view *sampler_view;
       struct pipe_sampler_state sampler = {0};
+      sampler.normalized_coords = true;
       const struct pipe_sampler_state *samplers[1] = {&sampler};
       unsigned level = texImage->TexObject->Attrib.MinLevel + texImage->Level;
       unsigned max_layer = util_max_layer(texture, level);
diff --git a/src/mesa/state_tracker/st_context.c b/src/mesa/state_tracker/st_context.c
index d3fc9774b76..c90927d053b 100644
--- a/src/mesa/state_tracker/st_context.c
+++ b/src/mesa/state_tracker/st_context.c
@@ -869,6 +869,9 @@ st_create_context(gl_api api, struct pipe_context *pipe,
    if (pipe->screen->get_param(pipe->screen, PIPE_CAP_INVALIDATE_BUFFER))
       ctx->has_invalidate_buffer = true;
 
+   if (pipe->screen->get_param(pipe->screen, PIPE_CAP_STRING_MARKER))
+      ctx->has_string_marker = true;
+
    st = st_create_context_priv(ctx, pipe, options);
    if (!st) {
       _mesa_free_context_data(ctx, true);
diff --git a/src/mesa/state_tracker/st_pbo_compute.c b/src/mesa/state_tracker/st_pbo_compute.c
index d88b444521a..23e659e4cd5 100644
--- a/src/mesa/state_tracker/st_pbo_compute.c
+++ b/src/mesa/state_tracker/st_pbo_compute.c
@@ -835,6 +835,7 @@ download_texture_compute(struct st_context *st,
       struct pipe_sampler_view templ;
       struct pipe_sampler_view *sampler_view;
       struct pipe_sampler_state sampler = {0};
+      sampler.normalized_coords = true;
       const struct pipe_sampler_state *samplers[1] = {&sampler};
       const struct util_format_description *desc = util_format_description(dst_format);
 
@@ -986,9 +987,9 @@ fail:
    /* Unbind all because st/mesa won't do it if the current shader doesn't
     * use them.
     */
-   pipe->set_sampler_views(pipe, PIPE_SHADER_COMPUTE, 0, 0, false,
+   pipe->set_sampler_views(pipe, PIPE_SHADER_COMPUTE, 0, 0,
                            st->state.num_sampler_views[PIPE_SHADER_COMPUTE],
-                           NULL);
+                           false, NULL);
    st->state.num_sampler_views[PIPE_SHADER_COMPUTE] = 0;
    pipe->set_shader_buffers(pipe, PIPE_SHADER_COMPUTE, 0, 1, NULL, 0);
 
diff --git a/src/util/macros.h b/src/util/macros.h
index 652ba63ac5a..88a4c519b17 100644
--- a/src/util/macros.h
+++ b/src/util/macros.h
@@ -140,7 +140,7 @@ do {                       \
  * value.  As a result, calls to it can be CSEed.  Note that using memory
  * pointed to by the arguments is not allowed for const functions.
  */
-#ifdef HAVE_FUNC_ATTRIBUTE_CONST
+#if !defined(__clang__) && defined(HAVE_FUNC_ATTRIBUTE_CONST)
 #define ATTRIBUTE_CONST __attribute__((__const__))
 #else
 #define ATTRIBUTE_CONST
diff --git a/src/util/u_atomic.h b/src/util/u_atomic.h
index 4f06979798a..9c236818767 100644
--- a/src/util/u_atomic.h
+++ b/src/util/u_atomic.h
@@ -76,6 +76,7 @@
  */
 #define p_atomic_cmpxchg(v, old, _new) \
    __sync_val_compare_and_swap((v), (old), (_new))
+#define p_atomic_cmpxchg_ptr(v, old, _new) p_atomic_cmpxchg(v, old, _new)
 
 #endif
 
@@ -100,6 +101,7 @@
 #define p_atomic_add_return(_v, _i) (*(_v) = *(_v) + (_i))
 #define p_atomic_fetch_add(_v, _i) (*(_v) = *(_v) + (_i), *(_v) - (_i))
 #define p_atomic_cmpxchg(_v, _old, _new) (*(_v) == (_old) ? (*(_v) = (_new), (_old)) : *(_v))
+#define p_atomic_cmpxchg_ptr(_v, _old, _new) p_atomic_cmpxchg(_v, _old, _new)
 
 #endif
 
@@ -174,6 +176,12 @@
    sizeof *(_v) == sizeof(__int64) ? InterlockedCompareExchange64 ((__int64 *)(_v), (__int64)(_new), (__int64)(_old)) : \
                                      (assert(!"should not get here"), 0))
 
+#if defined(_WIN64)
+#define p_atomic_cmpxchg_ptr(_v, _old, _new) (void *)InterlockedCompareExchange64((__int64 *)(_v), (__int64)(_new), (__int64)(_old))
+#else
+#define p_atomic_cmpxchg_ptr(_v, _old, _new) (void *)InterlockedCompareExchange((long *)(_v), (long)(_new), (long)(_old))
+#endif
+
 #define PIPE_NATIVE_ATOMIC_XCHG
 #define p_atomic_xchg(_v, _new) (\
    sizeof *(_v) == sizeof(long)    ? InterlockedExchange  ((long *)   (_v), (long)   (_new)) : \
@@ -255,6 +263,12 @@
    sizeof(*v) == sizeof(uint64_t) ? atomic_cas_64((uint64_t *)(v), (uint64_t)(old), (uint64_t)(_new)) : \
                                     (assert(!"should not get here"), 0))
 
+#if INTPTR_MAX == INT32_MAX
+#define p_atomic_cmpxchg_ptr(v, old, _new) (__typeof(*v))(atomic_cas_32((uint32_t *)(v), (uint32_t)(old), (uint32_t)(_new)))
+#else
+#define p_atomic_cmpxchg_ptr(v, old, _new) (__typeof(*v))(atomic_cas_64((uint64_t *)(v), (uint64_t)(old), (uint64_t)(_new)))
+#endif
+
 #endif
 
 #ifndef PIPE_ATOMIC
diff --git a/subprojects/libelf.wrap b/subprojects/libelf.wrap
deleted file mode 100644
index 5846f2dfc10..00000000000
--- a/subprojects/libelf.wrap
+++ /dev/null
@@ -1,6 +0,0 @@
-[wrap-git]
-directory = libelf-lfg-win32-1.1.0-freebsd-12.1.0
-
-url = https://github.com/LagFreeGames/libelf-lfg-win32.git
-revision = 1.1.0
-depth = 1